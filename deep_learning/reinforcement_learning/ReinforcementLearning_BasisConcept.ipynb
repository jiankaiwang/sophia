{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discounted Future Return\n",
    "\n",
    "$$R_t = \\sum^{T}_{k=0}\\gamma^{t}r_{t+k+1}$$\n",
    "\n",
    "$$R_0 = \\gamma^{0} * r_{1} + \\gamma^{1} * r_{2} = r_{1} + \\gamma^{1} * r_{2}\\ (while\\ T\\ =\\ 1) $$\n",
    "$$R_1 = \\gamma^{1} * r_{2} =\\ (while\\ T\\ =\\ 1) $$\n",
    "$$so,\\ R_0 = r_{1} + R_1$$\n",
    "\n",
    "Higher $\\gamma$ stands for lower discounted value, and lower $\\gamma$ stands for higher discounted value (in normal, $\\gamma$ value is between 0.97 and 0.99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.98):\n",
    "    discounted_returns = [0 for _ in rewards]\n",
    "    discounted_returns[-1] = rewards[-1]\n",
    "    for t in range(len(rewards)-2, -1, -1):\n",
    "        discounted_returns[t] = rewards[t] + discounted_returns[t+1]*gamma\n",
    "    return discounted_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If returns get higher when the time passes by, the Discounted Future Return method is not suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.8016, 5.92, 4]\n"
     ]
    }
   ],
   "source": [
    "print(discount_rewards([1,2,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If returns are the same or lesser when the time passes by, the Discounted Future Return method is suggested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.9404, 1.98, 1]\n"
     ]
    }
   ],
   "source": [
    "# about 2.94 fold\n",
    "# examples are like succeeding or failing\n",
    "print(discount_rewards([1,1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6503200000000002, 1.6840000000000002, 0.8]\n"
     ]
    }
   ],
   "source": [
    "# about 3.31 fold\n",
    "# examples are like relating to the time-consuming\n",
    "print(discount_rewards([1,0.9,0.8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and Exploit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\epsilon$-Greedy strategy\n",
    "\n",
    "Each time the agent decides to take an action, it will consider one of which, the recommended one (exploit) or the random one (explore). The value $\\epsilon$ standing for the probability of taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(action_distribution, epsilon=1e-1):\n",
    "    if random.random() < epsilon:\n",
    "        return np.argmax(np.random.random(action_distribution.shape))\n",
    "    else:\n",
    "        return np.argmax(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we assume there are 10 actions as well as  their probabilities (fixed probabilities on each step making us easier to monitor the result) for the agent to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25634599 0.61871756 0.76904048 0.01270285 0.79426894 0.63050946\n",
      "  0.36007282 0.21638714 0.89302613 0.18750439]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "action_distribution = np.random.random((1, 10))\n",
    "print(action_distribution)\n",
    "print(epsilon_greedy_action(action_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing $\\epsilon$-Greedy strategy\n",
    "\n",
    "At the beginning of training reinforcement learning, the agent knows nothing about the environment and the state or the feedback while taking an action as well. Thus we hope the agent takes more random actions (exploring) at the beginning of training. \n",
    "\n",
    "After a long training period, the agent knows the environment more and learns more the feedback given an action. Thus, we hope the agent takes an action based on its own experience (exploiting).\n",
    "\n",
    "We provide a new idea to anneal (or decay) the $\\epsilon$ parameter in each time the agent takes an action. The classic annealing strategy is decaying $\\epsilon$ value from 0.99 to 0.01 in around 10000 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_annealed(action_distribution, training_percentage, epsilon_start=1.0, epsilon_end=1e-2):\n",
    "    annealed_epsilon = epsilon_start * (1-training_percentage) + epsilon_end * training_percentage\n",
    "    if random.random() < annealed_epsilon:\n",
    "        # take random action\n",
    "        return np.argmax(np.random.random(action_distribution.shape))\n",
    "    else:\n",
    "        # take the recommended action\n",
    "        return np.argmax(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we assume there are 10 actions as well as  their probabilities (fixed probabilities on each step making us easier to monitor the result) for the agent to take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02100268 0.45219792 0.11096445 0.83215913 0.73601745 0.2042438\n",
      "  0.60744428 0.11792413 0.13162112 0.86374139]]\n",
      "percentage : 0.01 and action is 6\n",
      "percentage : 0.11 and action is 7\n",
      "percentage : 0.21 and action is 5\n",
      "percentage : 0.31 and action is 9\n",
      "percentage : 0.41 and action is 6\n",
      "percentage : 0.51 and action is 7\n",
      "percentage : 0.61 and action is 9\n",
      "percentage : 0.71 and action is 9\n",
      "percentage : 0.81 and action is 9\n",
      "percentage : 0.91 and action is 9\n"
     ]
    }
   ],
   "source": [
    "action_distribution = np.random.random((1, 10))\n",
    "print(action_distribution)\n",
    "\n",
    "for i in range(1, 99, 10):\n",
    "    percentage = i / 100.0\n",
    "    action = epsilon_greedy_annealed(action_distribution, percentage)\n",
    "    print(\"percentage : {} and action is {}\".format(percentage, action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Earn Max Returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Learning\n",
    "\n",
    "Policy learning is the policy the agent learning to earn the maximum returns. For instance, if we ride a bicycle, when the bicycle is tilt to the left we try to give more power to the right side. The above strategy is called policy learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Policy Learning\n",
    "\n",
    "$$arg\\ min_\\theta\\ -\\sum_{i}\\ R_{i}\\ log{p(y_{i}|x_{i}, \\theta)}$$\n",
    "\n",
    "$R_{i}$ is the discounted future return, $y_{i}$ is the action taken at time $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Learning\n",
    "\n",
    "Value learning is the agent learns the value from the state while taking an action. That is, value learning is to learn the value from a pair [state, action]. For example, if we ride a bicycle, we give higher or lower values to any combinations of [state, action], such a strategy is called value learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
