{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_Learning_Concept.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "hXEZh95vaXCS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Q-Learning\n",
        "\n",
        "* value learning : state + action \n",
        "* learn to find a max Q(s, a) (Q function calculates the max discounted future value, Q value) \n",
        "* Q value: the expected long-term rewards\n",
        "\n",
        "$$Q^*(s_t, a_t) = max_\\pi{E[\\sum^T_{i=t}\\gamma^ir^i]}$$\n",
        "\n",
        "* the chicken-and-egg conundrum"
      ]
    },
    {
      "metadata": {
        "id": "-vZgmRL7csy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bellman Function\n",
        "\n",
        "* redefine Q-value as the function of Q value to the future\n",
        "\n",
        "$$Q^*(s_t, a_t)=E[r_t + \\gamma{max_{a'}{Q^*(s_{t+1}, a')}}]$$\n",
        "\n",
        "* value iteration : the relation between (1) state + action and (2) Q value -> we are creating a Q-table (impractical)"
      ]
    },
    {
      "metadata": {
        "id": "wrffSwqOjdFE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Approximation to Q function\n",
        "\n",
        "Find a function approximated to Q function (similar with Q table)."
      ]
    },
    {
      "metadata": {
        "id": "RQRyRL3CkUA_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Q Network\n",
        "\n",
        "Its main purpose is using a deep neural network to estimate all possible' Q values given a state (e.g. an image).\n",
        "`Our loss function is designed to minimize the difference (value) between the Q value in this step and the approximated Q value in the next step.`\n",
        "\n",
        "**But this loss function would cause the training unstable due to the dependence between the adjacent two steps (codependency).**\n",
        "\n",
        "We can solve the `training instability` via **target Q-network** and **experience replay**."
      ]
    },
    {
      "metadata": {
        "id": "onirUUBZgrN0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Target Q-Network\n",
        "\n",
        "\n",
        "The loss function uses two Q values, $\\hat{Q}(S_t, a_t | \\theta)$ and $\\hat{Q}(S_{t+1}, a_{t+1} | \\theta)$. The first Q value is predicted by the **prediction network**, and the other one is using **target network**. The target network would be updated after several steps on the reference of the prediction network, this provided the stable parameter's update."
      ]
    },
    {
      "metadata": {
        "id": "18fgr20JimKk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Experience Replay\n",
        "\n",
        "Training using the adjacent steps also cause an unstable factor, the high relation. These two pairs (state + action) represent somehow correlation. Such a phenomenon is worse to train the model, we hope the batch's gradient can represent the whole gradient. But if the batch data cannot stand for the distribution of the whole data, the calculated gradient cannot as well.\n",
        "\n",
        "We can use experience replay to solve such problems. We collect the adjacent steps as $(s_i, a_i, r_i, s_{t+1})$ tuple and keep it in the pool. While training, we randomly select several tuples of them to train the model."
      ]
    },
    {
      "metadata": {
        "id": "qP5qm-OgnOtS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decision Making (Policy) based on Q functions\n",
        "\n",
        "Assume we have a best Q function, in each state we take all actions into the Q function, select the action whose Q value is the max one and take the action to get the new state.\n",
        "\n",
        "$$\\pi(s;\\theta) = arg\\ max_{a'}\\hat{Q^*}(s, a^{'}; \\theta)$$\n",
        "\n",
        "We can also make a random policy to make the model deviated from the Q function suggestion so as to explore the unknown action."
      ]
    },
    {
      "metadata": {
        "id": "VjEGFQ7jpjRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DQN vs. MDP\n",
        "\n",
        "DQN is also an MDP. The Markov Decision Process (MDP) key thought that the next state $s_{t+1}$ is only related with the current state $s_{t}, a_{t}$. DQN   solve the MDP issue via state history. DQN would use the latest several (e.g. four) states as the current state (input). Thus, we can use the time serial states."
      ]
    },
    {
      "metadata": {
        "id": "Wqd1wSeTSZd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}