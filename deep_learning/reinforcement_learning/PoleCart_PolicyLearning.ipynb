{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2DHksPpmsDN"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import gym\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6irOrJBQpMWD"
   },
   "source": [
    "# Rewards and $\\epsilon$-Greedy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wn0521YBpIcz"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.98):\n",
    "    discounted_returns = [0 for _ in rewards]\n",
    "    discounted_returns[-1] = rewards[-1]\n",
    "    for t in range(len(rewards)-2, -1, -1):\n",
    "        discounted_returns[t] = rewards[t] + discounted_returns[t+1]*gamma\n",
    "    return discounted_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TlUqjMHtpbYA"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(action_distribution, epsilon=1e-1):\n",
    "  if random.random() < epsilon:\n",
    "    return np.argmax(np.random.random(action_distribution.shape))\n",
    "  else:\n",
    "    return np.argmax(action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Rr9_-UVpdKm"
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_annealed(action_distribution, training_percentage, \n",
    "                            epsilon_start=1.0, epsilon_end=1e-2):\n",
    "  annealed_epsilon = epsilon_start * (1-training_percentage) + epsilon_end * training_percentage\n",
    "  if random.random() < annealed_epsilon:\n",
    "    # take random action\n",
    "    return np.argmax(np.random.random(action_distribution.shape))\n",
    "  else:\n",
    "    # take the recommended action\n",
    "    return np.argmax(action_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tjW2yA_Npf8L"
   },
   "source": [
    "# Create a Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQ2SiXEwpdmD"
   },
   "outputs": [],
   "source": [
    "\n",
    "class PGAgent(object):\n",
    "  def __init__(self, session, state_size, num_actions, \n",
    "               hidden_size, learning_rate=1e-3,\n",
    "               explore_exploit_setting='epsilon_greedy_annealed_1.0->0.001'):\n",
    "    \n",
    "    self.session = session\n",
    "    self.state_size = state_size\n",
    "    self.num_actions = num_actions\n",
    "    self.hidden_size = hidden_size\n",
    "    self.leanring_rate = learning_rate\n",
    "    self.explore_exploit_setting = explore_exploit_setting\n",
    "    \n",
    "    self.build_model()\n",
    "    #self.build_training()\n",
    "  \n",
    "  def build_model(self):\n",
    "    with tf.variable_scope(\"pg-model\") as scope:\n",
    "      self.state = tf.placeholder(shape=[None, self.state_size], dtype=tf.float32)\n",
    "      self.h0 = slim.fully_connected(self.state, self.hidden_size)\n",
    "      self.h1 = slim.fully_connected(self.h0, self.hidden_size)\n",
    "      \n",
    "      # output: (None, 2)\n",
    "      self.output = slim.fully_connected(self.h1,\n",
    "                                         self.num_actions,\n",
    "                                         activation_fn=tf.nn.softmax)\n",
    "      \n",
    "      scope.reuse_variables()\n",
    "  \n",
    "  def build_training(self, global_step):\n",
    "    self.action_input = tf.placeholder(shape=[None], dtype=tf.int32)\n",
    "    self.reward_input = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "    \n",
    "    # select the corresponding action\n",
    "    # * tf.shape(self.output)[1] is because we are going to reshape output \n",
    "    # into one-dimensional array, so we have to multiply column's number first\n",
    "    # second, we have to plus action_input to make choice\n",
    "    self.output_index_for_actions = (tf.range(0, tf.shape(self.output)[0]) * \n",
    "                                     tf.shape(self.output)[1]) + self.action_input\n",
    "                                     \n",
    "    # output (None, 2) reshape into (None * 2) by row first\n",
    "    self.logits_for_actions = tf.gather(tf.reshape(self.output, [-1]), \n",
    "                                        self.output_index_for_actions)\n",
    "    \n",
    "    # summarizing the above\n",
    "    # time-0's action might select output's first two value [9.89, 1.05, 7.6, 9.99, ...]\n",
    "    # and 9.89 stands for time-0 with action 0\n",
    "    # and 1.05 stands for time-0 with action 1\n",
    "    # and 7.6 stands for time-1 with action 0\n",
    "    # and 9.99 stands for time-1 with action 1    \n",
    "    # ...\n",
    "    \n",
    "    self.loss = - tf.reduce_mean(tf.log(self.logits_for_actions) * self.reward_input)\n",
    "    \n",
    "    self.optimizer = tf.train.AdamOptimizer(learning_rate=self.leanring_rate)\n",
    "    self.train_step = self.optimizer.minimize(self.loss, global_step=global_step)\n",
    "  \n",
    "  def sample_action_from_distribution(self, action_distribution, epsilon_percentage):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "      epsilon_percentage is the same with training_percentage\n",
    "      \n",
    "    Return:\n",
    "      the action id\n",
    "    \"\"\"\n",
    "    if self.explore_exploit_setting == \"epsilon_greedy_0.5\":\n",
    "      action = epsilon_greedy_action(action_distribution, 0.5)\n",
    "    elif self.explore_exploit_setting == \"epsilon_greedy_annealed_1.0->0.001\":\n",
    "      action = epsilon_greedy_annealed(action_distribution, epsilon_percentage,\n",
    "                                       1.0, 1e-3)\n",
    "    \n",
    "    return action\n",
    "  \n",
    "  def predict_action(self, state, epsilon_percentage):\n",
    "    action_distribution, = self.session.run(self.output, \n",
    "                                            feed_dict={self.state: [state]})\n",
    "    action = self.sample_action_from_distribution(action_distribution, \n",
    "                                                  epsilon_percentage)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XuCW8JVLq_qM"
   },
   "source": [
    "# Episode History and Memory List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ww9_cupq-Yt"
   },
   "outputs": [],
   "source": [
    "class EpisodeHistory(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.state_primes = []\n",
    "    self.discounted_returns = []\n",
    "\n",
    "  def add_to_history(self, state, action, reward, state_prime):\n",
    "    self.states.append(state)\n",
    "    self.actions.append(action)\n",
    "    self.rewards.append(reward)\n",
    "    self.state_primes.append(state_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bl4js0kIrHEc"
   },
   "outputs": [],
   "source": [
    "class Memory(object):\n",
    "  \n",
    "  def __init__(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.state_primes = []\n",
    "    self.discounted_returns = []\n",
    "    \n",
    "  def reset_memory(self):\n",
    "    self.states = []\n",
    "    self.actions = []\n",
    "    self.rewards = []\n",
    "    self.state_primes = []\n",
    "    self.discounted_returns = []\n",
    "\n",
    "  def add_episode(self, episode):    \n",
    "    self.states += episode.states\n",
    "    self.actions += episode.actions\n",
    "    self.rewards += episode.rewards\n",
    "    self.discounted_returns += episode.discounted_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiBGm5I3rLgs"
   },
   "source": [
    "# Gradient-Based Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x1oysFb6rKMt"
   },
   "outputs": [],
   "source": [
    "def main(full_training):\n",
    "  \n",
    "  # configurate the setting\n",
    "  total_episodes = 7001 if full_training else 10  # total episode (actions)\n",
    "  epsilon_stop = 3000 if full_training else 100  # for training percentage, e.g. 1 / 3000\n",
    "  \n",
    "  max_episode_length = 500 if full_training else 97  # explore or exploit times in each episodes\n",
    "  train_frequency = 8  # collect 8 units max_episode_length\n",
    "  \n",
    "  should_render = False  # show game on the screen\n",
    "  \n",
    "  explore_exploit_setting = \"epsilon_greedy_annealed_1.0->0.001\"\n",
    "  \n",
    "  env = gym.make('CartPole-v0')\n",
    "  state_size = env.observation_space.shape[0]  # here is 4\n",
    "  num_actions = env.action_space.n  # here is 2\n",
    "  \n",
    "  solved = False\n",
    "  \n",
    "  episode_rewards = []\n",
    "  batch_losses = []\n",
    "  \n",
    "  global_memory = Memory()\n",
    "  global_step = tf.Variable(0, name=\"gloabl_step\", trainable=False)\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "\n",
    "    agent = PGAgent(session=sess, state_size=state_size,\n",
    "                    num_actions=num_actions, hidden_size=16,\n",
    "                    explore_exploit_setting=explore_exploit_setting)\n",
    "    \n",
    "    agent.build_training(global_step)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    latest_indexing = 0\n",
    "    if os.path.exists(os.path.join(\".\",\"checkpoint\")):\n",
    "      latest_checkpoint = tf.train.latest_checkpoint(\"./\")\n",
    "      latest_indexing = int(latest_checkpoint.split(\"-\")[-1])\n",
    "      saver.restore(sess, latest_checkpoint)\n",
    "      print(\"Model was restored from {}.\".format(latest_checkpoint))\n",
    "    else:\n",
    "      sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # here we introduce a new variable latest_indexing\n",
    "    # because epsilon_percentage (training progress) is based on it\n",
    "    for i in tqdm.tqdm(range(latest_indexing, total_episodes)):\n",
    "      state = env.reset()\n",
    "      \n",
    "      episode_reward = 0.0\n",
    "      episode_history = EpisodeHistory()\n",
    "      epsilon_percentage = float(min(float(i) / epsilon_stop, 1.0))\n",
    "      \n",
    "      for j in range(max_episode_length):\n",
    "        # the block is mainly used to fetch action and reward data\n",
    "        \n",
    "        action = agent.predict_action(state, epsilon_percentage)\n",
    "        # state_prime: as Object, here is a list\n",
    "        # reward: as float in shape (1, )\n",
    "        # terminal: as bool in shape(1, )\n",
    "        state_prime, reward, terminal, _ = env.step(action)\n",
    "        \n",
    "        if solved and should_render:\n",
    "            env.render()\n",
    "        \n",
    "        episode_history.add_to_history(state, action, reward, state_prime)\n",
    "        state = state_prime\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if terminal:\n",
    "          # being True indicates the episode has terminated\n",
    "          episode_history.discounted_returns = discount_rewards(episode_history.rewards)\n",
    "          global_memory.add_episode(episode_history)\n",
    "          \n",
    "          if np.mod(i, train_frequency) == 0:\n",
    "            # start the training\n",
    "            feed_dict = {agent.action_input: np.array(global_memory.actions), \n",
    "                         agent.reward_input: np.array(global_memory.discounted_returns),\n",
    "                         agent.state: np.array(global_memory.states)}\n",
    "            _, batch_loss = sess.run([agent.train_step, agent.loss], feed_dict=feed_dict)\n",
    "            batch_losses.append(batch_loss)\n",
    "            global_memory.reset_memory()\n",
    "          \n",
    "          episode_rewards.append(episode_reward)\n",
    "          break\n",
    "       \n",
    "      if i % 10 == 0:\n",
    "        if np.mean(episode_rewards[:-100]) > 140.0:\n",
    "          solved = True  \n",
    "        else:\n",
    "          solved = False\n",
    "          \n",
    "      if i % 2000 == 0 and i > 0:\n",
    "        # save checkpoint\n",
    "        saver.save(sess, \"./cp-v0-checkpoint\", global_step=global_step)\n",
    "        print(\"Model was stored.\")\n",
    "            \n",
    "      if i % 500 == 0 and i > 0:\n",
    "        print('Solved: {}, Mean Reward: {}'.format(solved, np.mean(episode_rewards[:-100])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "id": "MGBW00YMrRHd",
    "outputId": "eb10563b-c975-439c-ed1f-381972a4ce5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "  0%|          | 0/7001 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "  7%|▋         | 511/7001 [00:06<01:29, 72.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 25.850374064837904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1001/7001 [00:14<01:41, 59.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 31.79134295227525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1508/7001 [00:26<02:11, 41.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 38.34760885082084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 2002/7001 [00:45<04:20, 19.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was stored.\n",
      "Solved: False, Mean Reward: 49.25092056812204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2502/7001 [01:19<05:29, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 69.69929196168263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 3003/7001 [01:56<05:16, 12.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 87.93898655635988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3502/7001 [02:32<03:56, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 101.18847397824169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 4002/7001 [03:04<03:31, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was stored.\n",
      "Solved: False, Mean Reward: 107.99923096641886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 4502/7001 [03:35<02:33, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 113.06725744149057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 5004/7001 [04:05<01:53, 17.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 116.21403795143848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 5503/7001 [04:34<01:28, 16.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 117.9877800407332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 6003/7001 [05:00<01:03, 15.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was stored.\n",
      "Solved: False, Mean Reward: 119.1308252838502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 6504/7001 [05:27<00:26, 18.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 119.82565224183722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7001/7001 [05:55<00:00, 18.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solved: False, Mean Reward: 120.51775105057239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "full_training = True\n",
    "main(full_training)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PoleCart_PolicyLearning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
