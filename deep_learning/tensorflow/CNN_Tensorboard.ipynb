{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQuickstart to Tensorboard\\nThe dataset is using MNIST handwritten dataset.\\n\\nauthor: Jiankai Wang\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quickstart to Tensorboard\n",
    "The dataset is using MNIST handwritten dataset.\n",
    "\n",
    "author: Jiankai Wang\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-62a56c969fbd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters / Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_training = True\n",
    "learning_rate = 1e-2\n",
    "training_epochs = 100 if full_training else 1\n",
    "batch_size = 64 if full_training else 32\n",
    "display_step = 1\n",
    "log_path = os.path.join(\"/\",\"tmp\",\"tensorboard_log\",\"mnist_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input, weight_shape, bias_shape):\n",
    "    in_count = weight_shape[0] * weight_shape[1] * weight_shape[2]\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2/in_count)**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    weight = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    bias = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    conv = tf.nn.conv2d(input, weight, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    return tf.nn.relu(tf.nn.bias_add(conv, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool2d(input, k=2):\n",
    "    return tf.nn.max_pool(value=input, ksize=[1,k,k,1], strides=[1,k,k,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense(input, weight_shape, bias_shape):\n",
    "    in_count = weight_shape[0] * weight_shape[1]\n",
    "    weight_init = tf.random_normal_initializer(stddev=(2/in_count)**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    weight = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    bias = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    logits = tf.matmul(input, weight)\n",
    "    return tf.nn.relu(tf.nn.bias_add(logits, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(input, keep_prob=0.5):\n",
    "    x = tf.reshape(input, [-1, 28, 28, 1])\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        conv_1 = conv2d(x, [3, 3, 1, 32], [32])         # 28 x 28 x 32\n",
    "        pool_1 = pool2d(conv_1, 2)                      # 14 x 14 x 32\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        conv_2 = conv2d(pool_1, [3, 3, 32, 64], [64])   # 14 x 14 x 64\n",
    "        pool_2 = pool2d(conv_2, 2)                      # 7 x 7 x 64\n",
    "        \n",
    "    with tf.variable_scope(\"fc\"):\n",
    "        pool_2_flat = tf.reshape(pool_2, [-1, 7 * 7 * 64])\n",
    "        fc_1 = dense(pool_2_flat, [7 * 7 * 64, 1024], [1024])\n",
    "        \n",
    "        # dropout\n",
    "        fc_1_dropout = tf.nn.dropout(fc_1, keep_prob=keep_prob)\n",
    "        \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = dense(fc_1_dropout, [1024, 10], [10])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    \"\"\"\n",
    "    output: the logits value from inference\n",
    "    y: the labeling data\n",
    "    \"\"\"\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, global_step):\n",
    "    \"\"\"\n",
    "    loss: the loss value\n",
    "    global_step: the global training step index\n",
    "    \"\"\"\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads, global_step=global_step)\n",
    "    return grads, apply_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    output: the logits value from inference\n",
    "    y: the labeling data\n",
    "    \"\"\"\n",
    "    compare = tf.equal(tf.argmax(output, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "    tf.summary.scalar(\"eval\", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 'tensorboard --logdir=/tmp/tensorboard_log/' to monitor the training process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Run 'tensorboard --logdir=/tmp/tensorboard_log/' to monitor the training process.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name mlp/hidden_1/W:0 is illegal; using mlp/hidden_1/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/b:0 is illegal; using mlp/hidden_1/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/W:0 is illegal; using mlp/hidden_2/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/b:0 is illegal; using mlp/hidden_2/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/fc/W:0 is illegal; using mlp/fc/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/fc/b:0 is illegal; using mlp/fc/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/output/W:0 is illegal; using mlp/output/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/output/b:0 is illegal; using mlp/output/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/W:0/gradient is illegal; using mlp/hidden_1/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/b:0/gradient is illegal; using mlp/hidden_1/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/W:0/gradient is illegal; using mlp/hidden_2/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/b:0/gradient is illegal; using mlp/hidden_2/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/fc/W:0/gradient is illegal; using mlp/fc/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/fc/b:0/gradient is illegal; using mlp/fc/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/output/W:0/gradient is illegal; using mlp/output/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/output/b:0/gradient is illegal; using mlp/output/b_0/gradient instead.\n",
      "Epoch: 1, Accuracy: 0.9300000071525574, Vaildation Error: 0.07\n",
      "Epoch: 2, Accuracy: 0.949999988079071, Vaildation Error: 0.05\n",
      "Epoch: 3, Accuracy: 0.9700000286102295, Vaildation Error: 0.03\n",
      "Epoch: 4, Accuracy: 0.9700000286102295, Vaildation Error: 0.03\n",
      "Epoch: 5, Accuracy: 0.9800000190734863, Vaildation Error: 0.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-855253f0fc9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mfeed_dict_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_opt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_data\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"mlp\"):\n",
    "        x = tf.placeholder(\"float\", [None, 784])  # x is batch input\n",
    "        y = tf.placeholder(\"float\", [None, 10])   # y is output for 10 classification\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "        output = inference(x, keep_prob=keep_prob)   # get the inference result\n",
    "        loss_val = loss(output=output, y=y)   # get the loss\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)   # training step\n",
    "        train_grads, train_opt = training(loss=loss_val, global_step=global_step)   # training body\n",
    "        eval_opt = evaluate(output=output, y=y)   # evaluation result\n",
    "        \n",
    "        # show all training variable info\n",
    "        # may cause summary name error\n",
    "        # INFO:tensorflow:Summary name mlp/hidden_1/W:0 is illegal; using mlp/hidden_1/W_0 instead.\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name, var)\n",
    "            \n",
    "        # show grads info\n",
    "        for grad, var in train_grads:\n",
    "            tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "        init_var = tf.global_variables_initializer()\n",
    "        summary_opt = tf.summary.merge_all()   # merge all summaries\n",
    "        saver = tf.train.Saver()   # for saving checkpoints\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter(log_path, graph=sess.graph)   # write the summary \n",
    "            sess.run(init_var)   # initialize all variables\n",
    "\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_loss = 0.\n",
    "                total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "                for idx in range(total_batch):\n",
    "                    batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)   # get the batch data\n",
    "\n",
    "                    feed_dict_data = {x: batch_x, y: batch_y, keep_prob: 0.5}\n",
    "                    grads, _ = sess.run([train_grads, train_opt], feed_dict=feed_dict_data)   # run training\n",
    "\n",
    "                    batch_loss = sess.run(loss_val, feed_dict=feed_dict_data)\n",
    "                    avg_loss += batch_loss / total_batch   # calculate the average loss\n",
    "\n",
    "                if epoch % display_step == 0:\n",
    "                    # record log\n",
    "\n",
    "                    feed_dict_val_data = {x: mnist.validation.images, y: mnist.validation.labels, keep_prob: 1.0}\n",
    "                    acc = sess.run(eval_opt, feed_dict=feed_dict_val_data)   # calculate the accuracy\n",
    "\n",
    "                    print(\"Epoch: {}, Accuracy: {}, Vaildation Error: {}\".format(epoch+1, round(acc,2), round(1-acc,2)))\n",
    "                    tf.summary.scalar(\"validation_accuracy\", acc)  \n",
    "\n",
    "                    summary_str = sess.run(summary_opt, feed_dict=feed_dict_val_data)\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))   # write out the summary\n",
    "\n",
    "                    saver.save(sess, os.path.join(log_path, \"model-checkpoint\"), global_step=global_step)\n",
    "\n",
    "            print(\"Training finishing.\")\n",
    "\n",
    "            feed_dict_test_data = {x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0}\n",
    "            acc = sess.run(eval_opt, feed_dict=feed_dict_test_data)   # test result\n",
    "            print(\"Test Accuracy:\",acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
