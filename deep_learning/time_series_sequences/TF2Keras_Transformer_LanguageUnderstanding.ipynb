{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF2Keras_Transformer_LanguageUnderstanding.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNFFXTXwP8lGOfbsm5tHqay"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XXlm_96N6Z35"},"source":["This tutorial guides you on how to train a `Transformer` model to translate from Portuguese to English.\n","\n","The core idea behind the `Transformer` mechanism is the `self-attention` - the ability to address different positions of the input sequence to compute a representation of that sequence. `Transformer` creates stacks of self-attention layers and is implemented as scaled dot product attention and multi-head attention. **More specifically, a Transformer model handles variable-sized input using stacks of self-attention instead of RNNs or CNNs**.\n","\n","There are several advantages to the Transformer model.\n","* It makes no assumptions about the temporal/spatial relationships across the data. (Idea for the set of objects)\n","* Layer outputs can be calculated in parallel, instead of a series, or dependent compute, like RNNs.\n","* Items with impacts can affect other's outputs without passing through multi-step RNN steps or CNN steps ([Scene Memory Transformer](https://arxiv.org/pdf/1903.03878.pdf)).\n","* It can learn long-term relationships or dependencies, this provides a solution for many sequence tasks.\n","\n","The downsides of the Transformer model.\n","* For a time or sequence data, the output at each time step is required to calculate from the entire history instead of the inputs and the hidden states only.\n","* If the input has the temporal/spatial relationship, like time or text, some positional encoding must be added or the model will more effectively see a bag of words.\n","\n","![](https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png)\n","\n","Refer to Tensorflow.org (2020) \n","\n","The following notebook shows how to build a Transformer unit from scratch. You can also use the latest API like `tf.keras.layers.MultiHeadAttention` for building Transformer."]},{"cell_type":"code","source":["!pip install -q -U \"tensorflow-text==2.8.*\""],"metadata":{"id":"ljIcOjm1VMB0","executionInfo":{"status":"ok","timestamp":1648789644655,"user_tz":-480,"elapsed":12885,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"84052999-cc35-4c56-a4ab-b567b0908d01"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 5.1 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 48.3 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"GN0FE_dZSZfs","outputId":"7a439945-fcc4-4a64-b1b8-76a2a542b3f1","executionInfo":{"status":"ok","timestamp":1648789650029,"user_tz":-480,"elapsed":5378,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["import logging\n","logging.basicConfig(level=logging.ERROR, \n","                    format=\"%(asctime)s - %(levelname)s : %(message)s\")\n","\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","import tensorflow as tf\n","log = logging.getLogger('tensorflow')\n","log.setLevel(logging.ERROR)\n","print(\"Tensorflow Version: {}\".format(tf.__version__))\n","print(\"GPU {} available.\".format(\"is\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not\"))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow Version: 2.8.0\n","GPU not available.\n"]}]},{"cell_type":"code","metadata":{"id":"8k9ZLiIhSvrZ","executionInfo":{"status":"ok","timestamp":1648789650030,"user_tz":-480,"elapsed":6,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGEhN4zbI20N"},"source":["# Setup Input Pipeline\n","\n","Here we load the [Portuguese-English translation dataset](https://github.com/neulab/word-embeddings-for-nmt) from the [TED Talks Open Translation Project](https://www.ted.com/participate/translate). This dataset contains 50000 training examples, 2000 test examples, and 1100 validation examples."]},{"cell_type":"code","metadata":{"id":"RIXI_enwS7cR"},"source":["examples, metadata = tfds.load(\"ted_hrlr_translate/pt_to_en\", with_info=True, as_supervised=True)\n","examples, metadata"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Show how many sequences in the dataset."],"metadata":{"id":"DBTKLXxWDVCB"}},{"cell_type":"code","metadata":{"id":"-XPw_sWrKJee","executionInfo":{"status":"aborted","timestamp":1648789652763,"user_tz":-480,"elapsed":518,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["\"Train: {}, Test: {}, Validation: {}\".format(\n","  metadata.splits['train'].num_examples, \n","  metadata.splits['test'].num_examples, \n","  metadata.splits['validation'].num_examples\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the training and validating datasets."],"metadata":{"id":"NcPj5v8UDcds"}},{"cell_type":"code","metadata":{"id":"gGvnpr5YJlYC","executionInfo":{"status":"aborted","timestamp":1648789652764,"user_tz":-480,"elapsed":519,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["train_examples, val_examples = examples[\"train\"], examples[\"validation\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h1EHLMurRad5","executionInfo":{"status":"aborted","timestamp":1648789652765,"user_tz":-480,"elapsed":520,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["for pt, en in train_examples.take(1):\n","  print(\"EN: {}\".format(en.numpy().decode('utf-8')))\n","  print(\"PT: {}\".format(pt.numpy().decode('utf-8')))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDkKzRTRQPoB"},"source":["We first create tokens from the text or the sequence. Two ways of creating tokens, one is to split it by the space, and the second is to break down the words into subwords."]},{"cell_type":"markdown","metadata":{"id":"oglhRcKHQ7m_"},"source":["## Text tokenization & detokenization\n","\n","Before sending the text into the model, you should tokenize the text to get the token ID list. The token ID is also the indices of the embedding."]},{"cell_type":"markdown","source":["### Word-based tokenization"],"metadata":{"id":"GzkGIdCWNtCK"}},{"cell_type":"code","metadata":{"id":"WirO9-zsK1ka","executionInfo":{"status":"aborted","timestamp":1648789652765,"user_tz":-480,"elapsed":519,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["tokens_en = tf.keras.preprocessing.text.Tokenizer(filters='')\n","tokens_en.fit_on_texts([en_data.numpy().decode(\"utf-8\") for _, en_data in train_examples])\n","len(tokens_en.word_index), [tokens_en.index_word[idx] for idx in range(1,11)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5RQSTmIOECQ","executionInfo":{"status":"aborted","timestamp":1648789652765,"user_tz":-480,"elapsed":519,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["en_data = [en_data.numpy().decode('utf-8') for _, en_data in train_examples.take(1)]\n","en_data_seq_list = tokens_en.texts_to_sequences(en_data)\n","print(en_data_seq_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4JCEetxKRUJo","executionInfo":{"status":"aborted","timestamp":1648789652766,"user_tz":-480,"elapsed":520,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["en_data_text = tokens_en.sequences_to_texts(en_data_seq_list)\n","en_data_text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00rAZzKhRGoC"},"source":["### Subword-based tokenization"]},{"cell_type":"markdown","metadata":{"id":"DSMHMdn1Ls_U"},"source":["Create a custom subwords tokenizer from the training dataset using the `tfds` APIs."]},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = 9000,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"id":"NWE7d2zGSEPu","executionInfo":{"status":"ok","timestamp":1648790794319,"user_tz":-480,"elapsed":471,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def write_vocab_file(file_path, vocab):\n","  \"\"\"Write down the vocabulary file.\"\"\"\n","  with open(file_path, \"w\") as fout:\n","    for token in vocab:\n","      print(token, file=fout)\n","\n","if not os.path.exists(\"pt.txt\"):\n","  pt_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_examples.map(lambda pt, en: pt).prefetch(10),\n","      **bert_vocab_args\n","  )\n","  write_vocab_file(\"pt.txt\", pt_vocab)\n","if not os.path.exists(\"en.txt\"):\n","  en_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_examples.map(lambda pt, en: en).prefetch(10),\n","      **bert_vocab_args\n","  )\n","  write_vocab_file(\"en.txt\", en_vocab)"],"metadata":{"id":"IHz-iUqUbeP5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if os.path.exists(\"pt.txt\"):\n","  pt_vocab = open(\"pt.txt\").read().split('\\n')\n","  print(\"Number of subword-based tokens for PT: {}\".format(len(pt_vocab)))\n","  print(pt_vocab[100:120])\n","\n","if os.path.exists(\"en.txt\"):\n","  en_vocab = open(\"en.txt\").read().split('\\n')\n","  print(\"Number of subword-based tokens for EN: {}\".format(len(en_vocab)))\n","  print(en_vocab[100:120])"],"metadata":{"id":"pctBDeH6cKWs","executionInfo":{"status":"ok","timestamp":1648790796724,"user_tz":-480,"elapsed":2,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["assert os.path.exists(\"pt.txt\")\n","tokenizer_pt = text.BertTokenizer(\"pt.txt\", **bert_tokenizer_params)\n","\n","assert os.path.exists(\"en.txt\")\n","tokenizer_en = text.BertTokenizer(\"en.txt\", **bert_tokenizer_params)"],"metadata":{"id":"EVXTAEfncVEP","executionInfo":{"status":"aborted","timestamp":1648789652768,"user_tz":-480,"elapsed":22,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXGvZswqNTUa","executionInfo":{"status":"aborted","timestamp":1648789652768,"user_tz":-480,"elapsed":22,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["sample_string = \"Transformer is nice.\"\n","\n","tokenized_string = tokenizer_en.tokenize(sample_string)\n","print(\"Tokenized String: {}\".format(tokenized_string))\n","\n","decoded_string = tokenizer_en.detokenize(tokenized_string)\n","print(\"Detokenized IDs: {}\".format(decoded_string))\n","decoded_string = decoded_string.merge_dims(outer_axis=-2, inner_axis=-1)\n","print(\"Merge: {}\".format(decoded_string))\n","print(\"String: {}\".format(\n","  ' '.join([v.decode('utf-8') for v in tf.squeeze(decoded_string, axis=0).numpy()])\n","))\n","\n","print(\"1761 -> {}\".format(tokenizer_en.detokenize([[1761]])))\n","print(\"227 -> {}\".format(tokenizer_en.detokenize([[227]])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwUqsnNYTGor"},"source":["Add a start and end token to the input and the target."]},{"cell_type":"code","metadata":{"id":"n547X4oOP-ig","executionInfo":{"status":"aborted","timestamp":1648789652768,"user_tz":-480,"elapsed":22,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  \"\"\"Add the [START] in the front of the string and [END] in the end of string.\"\"\"\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count, 1], START)\n","  ends = tf.fill([count, 1], END)\n","  return tf.concat([starts, ragged, ends], axis=1)\n","\n","def encode(lang1, lang2):\n","  \"\"\"Encode both of lang1 and lang2 into the indices.\"\"\"\n","  lang1 = add_start_end(tokenizer_pt.tokenize(lang1).merge_dims(-2, -1))\n","  lang2 = add_start_end(tokenizer_en.tokenize(lang2).merge_dims(-2, -1))\n","  return lang1, lang2"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for pt, en in train_examples.take(1):\n","  pt_, en_ = encode(pt, en)\n","  print(\"PT: {}\".format(pt_))\n","  print(\"detokenized PT: {}\".format(tokenizer_pt.detokenize(pt_)))\n","  print()\n","  print(\"EN: {}\".format(en_))\n","  print(\"detokenized EN: {}\".format(tokenizer_en.detokenize(en_)))"],"metadata":{"id":"7xcNX4W9WijG","executionInfo":{"status":"aborted","timestamp":1648789652769,"user_tz":-480,"elapsed":23,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Pipeline"],"metadata":{"id":"KXGr7j0nY6ZB"}},{"cell_type":"markdown","metadata":{"id":"Zi3pIHIIVAu0"},"source":["To keep the dataset small and training fast, drop examples with a length of over 40 tokens. "]},{"cell_type":"code","metadata":{"id":"w4hkGAHjU2-r","executionInfo":{"status":"aborted","timestamp":1648789652769,"user_tz":-480,"elapsed":23,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["MAX_LENGTH = 40"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOiAPrFhVOt6","executionInfo":{"status":"aborted","timestamp":1648789652769,"user_tz":-480,"elapsed":22,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  \"\"\"That only both of x and y are less than MAX_LENGTH is allowed being datasets.\"\"\"\n","  return tf.logical_and(tf.size(x) <= max_length, tf.size(y) <= max_length)\n","\n","x = tf.constant(tf.range(30))\n","y = tf.constant(tf.range(90))\n","filter_max_length(x, y).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlYdPm7dVkro"},"source":["Build the input pipeline."]},{"cell_type":"code","metadata":{"id":"7JAwycJaViS3","executionInfo":{"status":"aborted","timestamp":1648789652770,"user_tz":-480,"elapsed":23,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64\n","\n","def tokenize_pairs(pt, en):\n","  \"\"\"Convert from ragged to dense, padding with zeros.\"\"\"\n","  pt = tf.squeeze(pt.to_tensor(), axis=1)\n","  en = tf.squeeze(en.to_tensor(), axis=1)\n","  return pt, en\n","\n","train_dataset = train_examples.map(encode)\\\n","  .filter(filter_max_length)\\\n","  .cache()\\\n","  .shuffle(BUFFER_SIZE)\\\n","  .batch(BATCH_SIZE)\\\n","  .map(tokenize_pairs)\\\n","  .prefetch(tf.data.experimental.AUTOTUNE)\n","\n","val_dataset = val_examples.map(encode)\\\n","  .filter(filter_max_length)\\\n","  .batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"executionInfo":{"status":"aborted","timestamp":1648789652770,"user_tz":-480,"elapsed":23,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}},"id":"aoXBLKmgdgQc"},"source":["pt_batch, en_batch = next(iter(train_dataset))\n","pt_batch[0], en_batch[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SbVuOvE6KUBA"},"source":["# Positional Encoding\n","\n","The transformer model does not contain any recurrent or convolution unit, it requires the `positional encoding` to give the model additional information about the relative position of words in the sentence.\n","\n","**The basic idea of the positional encoding is to add it to the embedding vector**. Embeddings represent a token (words or subwords) in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embedding doesn't contain the relative position of words in a sentence. After adding the position encoding vector, tokens will be closer to the similar one based on their meanings and their positions in the sentence, in its d-dimensional space.\n","\n","More details about the [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb).\n","\n","The formula for calculating the positional encoding.\n","\n","$$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$\n","$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$$"]},{"cell_type":"code","metadata":{"id":"gwcH_beHWe0y","executionInfo":{"status":"aborted","timestamp":1648789652770,"user_tz":-480,"elapsed":23,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def get_angles(pos, i, d_model):\n","  \"\"\"Calculate the angles.\"\"\"\n","  angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","  return pos * angle_rates  # (pos, d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8uImEPvwabaN"},"source":["Use the number of dimensions (d_model) to represent each position that is summed equal to the value of pos."]},{"cell_type":"code","metadata":{"id":"iV2xXmphSAWN","executionInfo":{"status":"aborted","timestamp":1648789652771,"user_tz":-480,"elapsed":24,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def positional_encoding(position, d_model):\n","  \"\"\"\n","  Args:\n","    position: the number of positions in shape (p,)\n","    d_model: the dimension of the model in shape (d,)\n","  \n","  Returns:\n","    (1, position, d_position)\n","  \"\"\"\n","  # to assemble the new matrix in shape (position, d_model)\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n","                          np.arange(d_model)[np.newaxis, :], \n","                          d_model)\n","  \n","  # apply sin to even indices in the array (2i)\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","  # apply cos to odd indices in the array (2i+1)\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  angle_rads = tf.expand_dims(angle_rads, axis=0)\n","\n","  return tf.cast(angle_rads, tf.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEcKd0CqT2Sd","executionInfo":{"status":"aborted","timestamp":1648789652771,"user_tz":-480,"elapsed":24,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["pos_encoding = positional_encoding(100, 512)\n","print(pos_encoding.shape)\n","\n","plt.pcolormesh(pos_encoding[0], cmap=\"RdBu\")\n","plt.title(\"Positional Encoding\")\n","plt.xlabel(\"Depth (Dimension)\")\n","plt.xlim([0, 512])\n","plt.ylabel(\"Position\")\n","plt.colorbar()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zgtuDRBjZau5"},"source":["# Masking"]},{"cell_type":"markdown","metadata":{"id":"2wQ2SH4fBuML"},"source":["## Padded Mask\n","\n","Mask all pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where `pad` value 0 is present, it outputs a `1` at those locations, and a `0` otherwise. "]},{"cell_type":"code","metadata":{"id":"bKudnAFdT90j","executionInfo":{"status":"aborted","timestamp":1648789652771,"user_tz":-480,"elapsed":24,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def create_padding_mask(seq):\n","  \"\"\"\n","  Args:\n","    seq: (batch_size, sequence_length)\n","\n","  Returns: (batc_size, 1, 1, sequence_length)\n","  \"\"\"\n","  seq_ = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","\n","  # add extra dimensions to add the padding information\n","  return seq_[:, tf.newaxis, tf.newaxis, :]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qug_dshBbND1","executionInfo":{"status":"aborted","timestamp":1648789652772,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["x = tf.constant([[2, 7, 7, 3, 0, 0, 0], [2, 10, 20, 30, 3, 0, 0]])\n","x.numpy(), create_padding_mask(x).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6rWNDe30cqad"},"source":["## Look-ahead Mask\n","\n","The `look-ahead` mask is used to mask the future tokens in a sequence. For example, to predict the third token only the first and the second token will be used."]},{"cell_type":"code","metadata":{"id":"7WoqQ_qHbZH_","executionInfo":{"status":"aborted","timestamp":1648789652772,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def create_look_ahead_mask(size):\n","  \"\"\"Create the look ahead mask.\"\"\"\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask # (seq_len, seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0KPJkrceQCc","executionInfo":{"status":"aborted","timestamp":1648789652772,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["x = tf.random.uniform((1, 5))\n","create_look_ahead_mask(x.shape[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4mbhJVuya5nZ"},"source":["# Scaled Dot Product Attention"]},{"cell_type":"markdown","metadata":{"id":"Jh9CUtEEcgPS"},"source":["![](https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png)\n","\n","Refer to Tensorflow.org (2020).\n","\n","The attention function used by a transformer takes three inputs, Q (Query), K (Key), and V (Value). The formula to calculate the attention weights is:\n","\n","$$Attention(Q,K,V)=softmax_K(\\frac{QK^T}{\\sqrt{d_K}})V$$\n","\n","- The query in the additive attention is the deocder input.\n","- The key and value in the additive attention are the encoder output and state. \n","\n","The dot-product attention is scaled by a factor of square root of the depth. This is quite smart if for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting on a hard softmax. \n","\n","Consider that Q and K have a mean of 0 and variance of 1. The matrix multiplication will have a mean of 0 and variance of dk, so that the square root of dk is used for scaling."]},{"cell_type":"code","metadata":{"id":"Bp9UAYE_ea57","executionInfo":{"status":"aborted","timestamp":1648789652773,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculated the attention weights.\n","  q, k, v: have the same leading dimensions\n","  q, k: have the same last dimensions\n","  k, v: have the same penultimate dimension, for example, seq_len_k == seq_len_v\n","  mask: has different shapes depending on its type (padding or look head)\n","\n","  Args:\n","    q (query): shape (..., seq_len_q, depth)\n","    k (key): shape (..., seq_len_k == encoder_timestamp, depth)\n","    v (value): shape (..., seq_len_v == encoder_timestamp, depth_v)\n","    mask: default is None,\n","          or Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k)\n","\n","  Returns:\n","    output, attention_weight\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","\n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  # -1e-9, not 0 for softmax\n","\n","  # softmax is on the last axis (seq_len_k) so that the scores add up to 1\n","  attention_weight = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weight, v)  #(..., seq_len_q, seq_len_v)\n","  return output, attention_weight"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssAy14ffnyWq"},"source":["After the softmax normalization is done on K, its value represents the amount of importance given to Q. In the end, the matrix multiplication represents the words you want to focus on are kept as-is and the irrelevant ones are flushed out."]},{"cell_type":"code","metadata":{"id":"_e-Xa5VhjwIy","executionInfo":{"status":"aborted","timestamp":1648789652773,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def print_output_shape(q, k, v):\n","  \"\"\"Check the output shape from the scaled dot product attention.\"\"\"\n","  temp_out, temp_weights = scaled_dot_product_attention(q, k, v, None)\n","  print(\"Output:\", temp_out)\n","  print(\"Attention Weights:\", temp_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWqbVAnEqK5Z","executionInfo":{"status":"aborted","timestamp":1648789652773,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["np.set_printoptions(suppress=True)\n","\n","temp_K = tf.constant([[10, 0, 0, 0], \n","                      [0, 10, 0, 0], \n","                      [0, 0, 10, 0], \n","                      [0, 0, 0, 10]], dtype=tf.float32)  # (1, 4, 4)\n","\n","temp_V = tf.constant([[10, 1], \n","                      [100, 10], \n","                      [1000, 100], \n","                      [10000, 1000]], dtype=tf.float32)  # (1, 4, 2)\n","\n","temp_Q = tf.constant([[0, 0, 1, 0], \n","                      [0, 1, 0, 0]], dtype=tf.float32)  # (1, 2, 4)\n","\n","print_output_shape(temp_Q, temp_K, temp_V)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"73CoSi1EyEUa"},"source":["**After the above validation, you will find out the attention weights are the relations between the query and keys. That is, the relations between the outputs (decoder) and inputs (encoder).** Let's calculate step by step."]},{"cell_type":"code","metadata":{"executionInfo":{"status":"aborted","timestamp":1648789652774,"user_tz":-480,"elapsed":26,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}},"id":"XWWXLwVtwS22"},"source":["temp_QK = tf.matmul(temp_Q, temp_K, transpose_b=True)  # (1, 2, 4)\n","print(temp_QK.shape, temp_QK)\n","\n","scaled_QK = temp_QK / tf.math.sqrt(tf.cast(tf.shape(temp_K)[-1], tf.float32))  # (1, 2, 4)\n","print(scaled_QK)\n","\n","softmax_QK = tf.nn.softmax(scaled_QK, axis=-1)  # (1, 2, 4)\n","print(softmax_QK)\n","\n","tf.matmul(softmax_QK.numpy(), temp_V)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"91zOmm3kEhgj"},"source":["# Multi-Head Attention\n","\n","![](https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png)\n","\n","Tensorflow.org (2020)\n","\n","Each attention block gets three inputs: V(Value), K(Key), and Q(Query). These are put through the linear layer before the scaled dot-product attention.\n","\n","In the following, we use a simplied dense layer for those inputs with `num_heads` to represent the number of many heads (`h` in the diagram). In the following, we also split the dimension with heads for simplicity. The output shape would be `[batch, num_heads, ...]` before applying the attention function."]},{"cell_type":"code","metadata":{"id":"rgd0gdfiu339","executionInfo":{"status":"aborted","timestamp":1648789652774,"user_tz":-480,"elapsed":26,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","\n","    assert d_model % num_heads == 0\n","\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","    self.d_depth = d_model // num_heads\n","\n","    self.wq = tf.keras.layers.Dense(units=d_model)\n","    self.wk = tf.keras.layers.Dense(units=d_model)\n","    self.wv = tf.keras.layers.Dense(units=d_model)\n","\n","    self.dense = tf.keras.layers.Dense(units=d_model)\n","\n","  def split_head(self, x, batch_size):\n","    x = tf.reshape(x, shape=(batch_size, -1, self.num_heads, self.d_depth))\n","\n","    # change to (batch_size, num_heads, seq_len, d_depth)\n","    x = tf.transpose(x, perm=[0, 2, 1, 3])\n","    return x\n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n","\n","    q = self.split_head(q, batch_size)  # (batch_size, num_heads, seq_len_q, d_depth)\n","    k = self.split_head(k, batch_size)  # (batch_size, num_heads, seq_len_k, d_depth)\n","    v = self.split_head(v, batch_size)  # (batch_size, num_heads, seq_len_v, d_depth)\n","\n","    # output: (batch_size, num_heads, seq_len_q, seq_len_v == d_depth)\n","    #         seq_len_v == d_depth is from self.qv()\n","    # attention: (batch_size, num_heads, seq_len_q, seq_len_k)\n","    output, attention = scaled_dot_product_attention(q=q, k=k, v=v, mask=mask)\n","\n","    # concatenation\n","    scaled_output = tf.transpose(output, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, d_depth)\n","    concat_output = tf.reshape(scaled_output, shape=(batch_size, -1, self.d_model))\n","\n","    output = self.dense(concat_output)  # (batch_size, seq_len_q, d_model)\n","    return output, attention"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"woYchRUTYfUB"},"source":["Create a `MultiHeadAttention` layer to try out."]},{"cell_type":"code","metadata":{"id":"rAYKpODAW-k9","executionInfo":{"status":"aborted","timestamp":1648789652774,"user_tz":-480,"elapsed":25,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n","x = tf.random.uniform(shape=(10, 50, 256))  # (batch_size, seq_length, embedding + positional)\n","temp_outputs, temp_attentions = temp_mha(v=x, k=x, q=x, mask=None)\n","\n","# temp_outputs: (batch_size=10, seq_len=50, d_model=512)\n","# temp_attentions: (batch_size=10, num_heads=8, seq_len_q=50, seq_len_k=50)\n","temp_outputs.shape, temp_attentions.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67Iq9UpsZ_Wo"},"source":["# Point Wise Feed Forward Network\n","\n","The feed forward network is used after the multi-head attention for summarizing. The encoder's FNN is also the hidden state for the decoder."]},{"cell_type":"code","metadata":{"id":"4bBN8ABGZv0s","executionInfo":{"status":"aborted","timestamp":1648789652775,"user_tz":-480,"elapsed":26,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","    tf.keras.layers.Dense(units=dff, activation='relu'),\n","    tf.keras.layers.Dense(units=d_model)\n","  ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8kpZGMljbDft","executionInfo":{"status":"aborted","timestamp":1648789652775,"user_tz":-480,"elapsed":26,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["point_wise = point_wise_feed_forward_network(d_model=512, dff=2048)\n","point_wise(tf.random.uniform(shape=(32, 64, 256))).shape   # (32, 64, 256) -> (32, 64, 512)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RXnmo62Mbvyw"},"source":["# Encoder and Decoder\n","\n","![](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)\n","\n","Tensorflow.org (2020)\n","\n","The transformer model also adopts the encoder-decoder architecture. \n","- The input squence is passed through N encoder layers that generates an output for each token in the sequence.\n","- The decoder attends encoder's output and its own inputs (self-attention) to predict the next word."]},{"cell_type":"markdown","metadata":{"id":"is-yAPqsbYpr"},"source":["## Encoder Layer\n","\n","An encoder consists of two sublayers.\n","* A multi-head attention (with padded mask).\n","* A point-wise feed-forward network.\n","\n","Each of these encoder layer consists of the residual connections for avoid gradient vanishing. Each of sublayers above is passed through the `LayerNormalization`. The normalization is done on `d_model` axis."]},{"cell_type":"code","metadata":{"id":"UwX7KPtf5F26","executionInfo":{"status":"aborted","timestamp":1648789652777,"user_tz":-480,"elapsed":27,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  \"\"\"Implement the encoder layer.\"\"\"\n","\n","  def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model=d_model, dff=dff)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)    \n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self, x, training, mask):\n","    \"\"\"\n","    x: (batch_size, seq_len_q, d_model) over positional encoding\n","    \"\"\"\n","    \n","    # mha_output:     e.g. (batch_size=10, seq_len=50, d_model=512)\n","    # mha_attentions: e.g. (batch_size=10, num_heads=8, seq_len_q=50, seq_len_k=50)\n","\n","    mha_output, _ = self.mha(v=x, k=x, q=x, mask=mask) \n","    dropout_output1 = self.dropout1(mha_output, training=training)\n","    output1 = self.layernorm1(x + dropout_output1) # (batch_size, seq_len_q, d_model)\n","\n","    ffn_output = self.ffn(output1)\n","    dropout_output2 = self.dropout2(ffn_output, training=training)\n","    output2 = self.layernorm2(output1 + dropout_output2)  # (batch_size, seq_len_q, d_model)\n","    \n","    return output2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's build a encoder layer with an example first."],"metadata":{"id":"zFycCAnv8RnD"}},{"cell_type":"code","metadata":{"id":"PWCOh7s8f1w1","executionInfo":{"status":"aborted","timestamp":1648789652777,"user_tz":-480,"elapsed":27,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["temp_encoder_layer = EncoderLayer(512, 8, 2048, 0.1)\n","temp_encoder_layer_output = temp_encoder_layer(\n","  tf.random.normal(shape=(32, 36, 512)), training=False, mask=None  # (batch_size=32, seq_len_q=36, d_model=512)\n",")\n","temp_encoder_layer_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c1CZ8G0fgi2t"},"source":["## Decoder Layer\n","\n","A decoder layer consists of three sublayers.\n","* A masked multi-head attention layer (with a padded mask and a look ahead mask).\n","* A multi-head attention layer (with a padded mask).\n","* A point-wise feed-forward network.\n","\n","The second multi-head attention layer receives the output and value from the encoder as the key (K) and value (V). It also receives the query (Q) from the the first multi-head attention layer. This attention layer does the prediction by watching the output from the encoder and the current input from decoder."]},{"cell_type":"code","metadata":{"id":"2hlYoM6vgae9","executionInfo":{"status":"aborted","timestamp":1648789652777,"user_tz":-480,"elapsed":27,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  \"\"\"Implement the deocder layer.\"\"\"\n","  \n","  def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n","    self.dropout1 = tf.keras.layers.Dropout(rate=dropout_rate)\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n","    self.dropout2 = tf.keras.layers.Dropout(rate=dropout_rate)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model=d_model, dff=dff)\n","    self.dropout3 = tf.keras.layers.Dropout(rate=dropout_rate)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self, x, encoder_output, training, look_ahead_mask, padded_mask):\n","    # x: (batch_size, seq_len_q, d_model)\n","    # encoder_output: (batch_size, seq_len_q, d_model)\n","\n","    # mha1_output: (batch_size, seq_len_q, d_model)\n","    # mha1_attention: (batch_size, num_heads, seq_len_q, seq_len_k)\n","    mha1_output, mha1_attention = self.mha1(v=x, k=x, q=x, mask=look_ahead_mask)\n","    mha1_dropout = self.dropout1(mha1_output, training=training)\n","    output1 = self.layernorm1(x + mha1_dropout)  # (batch_size, seq_len_q, d_model)     \n","\n","    # mha2_output: (batch_size, seq_len_q, d_model)\n","    # mha2_attention: (batch_size, num_heads, seq_len_q, seq_len_q)\n","    mha2_output, mha2_attention = self.mha2(\n","      v=encoder_output, k=encoder_output, q=output1, mask=padded_mask)\n","    mha2_dropout = self.dropout2(mha2_output, training=training)\n","    output2 = self.layernorm2(output1 + mha2_dropout)  # (batch_size, seq_len_q, d_model)\n","\n","    ffn_output = self.ffn(output2)  # (batch_size, seq_len_q, d_model)\n","    ffn_dropout = self.dropout3(ffn_output, training=training)\n","    output3 = self.layernorm3(output2 + ffn_dropout)  # (batch_size, seq_len_q, d_model)\n","\n","    return output3, mha1_attention, mha2_attention"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try the decoder layer."],"metadata":{"id":"5N7mqR0KCDFS"}},{"cell_type":"code","metadata":{"id":"mllCf9kQsexH","executionInfo":{"status":"aborted","timestamp":1648789652778,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["temp_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n","temp_deocder_layer_output, temp_mha1_attn, temp_mha2_attn = temp_decoder_layer(\n","  x = tf.random.normal(shape=(32, 36, 512)),\n","  encoder_output = temp_encoder_layer_output,\n","  training=False,\n","  look_ahead_mask=None,\n","  padded_mask=None\n",")\n","\n","\"\"\"\n","temp_deocder_layer_output: (batch_size=32, seq_len_q=30, d_model=512)\n","temp_mha1_attn: (batch_size=32, num_heads=8, seq_len_q=30, seq_len_k=30)\n","temp_mha2_attn: (batch_size=32, num_heads=8, seq_len_q=30, seq_len_q=30)\n","\"\"\"\n","temp_deocder_layer_output.shape, temp_mha1_attn.shape, temp_mha2_attn.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"frq6kKGXbCaz"},"source":["**The attention weights in the decoder layer are focusing on itself (the button layer, the look-ahead mask), and focusing on the encoder output (the upper layer, the padded mask).**"]},{"cell_type":"markdown","metadata":{"id":"CIcfuEB39uO4"},"source":["## Encoder"]},{"cell_type":"markdown","metadata":{"id":"quQe1Ic7_L5Y"},"source":["An encoder consists of:\n","* Input Embedding\n","* Positional Encoding\n","* N times Encoder layers\n","\n","The encoder sums an input embedding and a positional encoding as the input to the encoder. The encoder's output would be the input of the decoder."]},{"cell_type":"code","metadata":{"id":"s1ixGdDDtQbV","executionInfo":{"status":"aborted","timestamp":1648789652778,"user_tz":-480,"elapsed":27,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               max_positional_encoding, dropout_rate=0.1):\n","    \"\"\"\n","    Args:\n","      num_layers: the number of decoder layer\n","      num_heads: the number of heads\n","      d_model: the dimension of multi-heads, fnn, embedding layer, positional encoding\n","      dff: the dimension in point wise feed forwaed network\n","      input_vocab_size: the vocab size of subwords for word embedding\n","      max_positional_encoding: the max number of the sequence length,\n","                               it can be assigned as the input_vocab_size\n","      dropout_rate: for the dropout layer\n","    \"\"\"\n","    super(Encoder, self).__init__()\n","\n","    self.embedding = tf.keras.layers.Embedding(\n","        input_dim=input_vocab_size, \n","        output_dim=d_model)\n","    self.positions = positional_encoding(\n","        position=max_positional_encoding, \n","        d_model=d_model)\n","    self.encoders = [\n","        EncoderLayer(\n","            d_model=d_model, \n","            num_heads=num_heads, \n","            dff=dff, \n","            dropout_rate=dropout_rate) \\\n","        for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate=dropout_rate)\n","\n","    # used in the call function\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","  def call(self, x, training, mask):\n","    \"\"\"\n","    Args:\n","      x: the input in shape (batch_size, seq_len)\n","      training: True ot False\n","      mask: the padded mask\n","    Returns:\n","      outputs: the input of the decoder in shape (batch_size, x_seq_len, d_model)\n","    \"\"\"\n","    embedding = self.embedding(x)  # (batch_size, x_seq_len, d_model)\n","    \n","    # scaled the embedding value for adding positional encoding\n","    embedding *= tf.sqrt(tf.cast(self.d_model, tf.float32))\n","\n","    # get the sequence length for positional encoding\n","    input_seq_len = tf.shape(x)[1]\n","    pos_embed = embedding + self.positions[:, :input_seq_len, :]\n","\n","    pos_embed = self.dropout(pos_embed, training=training)\n","    x_ = pos_embed\n","\n","    # N x EncoderLayers\n","    for idx in range(self.num_layers):\n","      x_ = self.encoders[idx](x_, training, mask)\n","\n","    return x_  # (batch_size, x_seq_len, d_model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try the enconder first."],"metadata":{"id":"yShzg9CUFvZx"}},{"cell_type":"code","metadata":{"id":"7r_lywioJPJj","executionInfo":{"status":"aborted","timestamp":1648789652778,"user_tz":-480,"elapsed":27,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["simple_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, dff=2048, \n","                         input_vocab_size=len(pt_vocab), \n","                         max_positional_encoding=10000)\n","\n","# generate the example batch input sequences\n","temp_input = tf.random.uniform(shape=(64, 62), dtype=tf.int64, minval=0, maxval=200)\n","\n","encoder_output = simple_encoder(temp_input, training=False, mask=None)\n","encoder_output.shape  # (batch_size=64, seq_len=62, d_model=512)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0kRVS9DrTs7-"},"source":["## Decoder"]},{"cell_type":"code","metadata":{"id":"y-5L5yOWJ6wA","executionInfo":{"status":"aborted","timestamp":1648789652779,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class Decoder(tf.keras.layers.Layer):\n","  \"\"\"Implement the deocder.\"\"\"\n","\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n","               max_positional_encoding, dropout_rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    # [batch_size, seq_len, d_model]\n","    self.embedding = tf.keras.layers.Embedding(\n","        input_dim=target_vocab_size, \n","        output_dim=d_model)\n","    # [1, num_position, d_model]\n","    self.positions = positional_encoding(\n","        position=max_positional_encoding, \n","        d_model=d_model)\n","    self.dropout = tf.keras.layers.Dropout(\n","        rate=dropout_rate)\n","    self.decoders = [\n","        DecoderLayer(d_model=d_model, \n","                     num_heads=num_heads, \n","                     dff=dff, \n","                     dropout_rate=dropout_rate) \\\n","        for _ in range(num_layers)]\n","\n","    # for the function call\n","    self.num_layers = num_layers\n","    self.d_model = d_model\n","\n","  def call(self, x, encoder_output, training, look_ahead_mask, mask):\n","    \"\"\"\n","    Args:\n","      x: the target, here is English, in shape (batch_size, seq_len)\n","      encoder_output: the encoder output in shape (batch_size, encoder_input_seq_len, d_model)\n","      training: True or False\n","      look_ahead_mask: (batch_size, seq_len, seq_len)\n","      mask: the padded mask in shape (batch_size, 1, 1, decoder_input_seq_len)\n","\n","    Returns:\n","      output: the decoder output in shape (batch_szie, decoder_input_seq_len, d_model)\n","      attentions: a dictionary for two stage attention weights\n","    \"\"\"\n","    attentions = {}   # for returns\n","\n","    embed = self.embedding(x)  # (batch_size, seq_len, d_model)\n","    # scaled for adding positional encoding\n","    embed *= tf.sqrt(tf.cast(self.d_model, tf.float32))  \n","\n","    # add the positional information\n","    input_seq_len = tf.shape(x)[1]\n","    pos_embed = embed + self.positions[:, :input_seq_len, :]\n","\n","    x_ = self.dropout(pos_embed, training=training)\n","\n","    # decoder layers\n","    for idx in range(self.num_layers):\n","      x_, attn1, attn2 = self.decoders[idx](x_, \n","                                            encoder_output=encoder_output, \n","                                            training=training, \n","                                            look_ahead_mask=look_ahead_mask, \n","                                            padded_mask=mask)\n","      attentions['decoder_layer{}_block1'.format(idx)] = attn1\n","      attentions['decoder_layer{}_block2'.format(idx)] = attn2\n","\n","    return x_, attentions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's try the decoder."],"metadata":{"id":"izX2egoKJvyN"}},{"cell_type":"code","metadata":{"id":"UYH95-tBZD_k","executionInfo":{"status":"aborted","timestamp":1648789652779,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["simple_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, dff=2048, \n","                         target_vocab_size=len(en_vocab), \n","                         max_positional_encoding=10000)\n","\n","temp_decoder_input = tf.random.uniform(shape=(64, 68), minval=0, maxval=200)\n","outputs, attentions = simple_decoder(x=temp_decoder_input, \n","                                     encoder_output=encoder_output, \n","                                     training=False, \n","                                     look_ahead_mask=None, \n","                                     mask=None)\n","\n","print(outputs.shape)  # (batch_size=64, decoder_input_seq_len=68, d_model=512)\n","for key, val in attentions.items():\n","  print(\"Key: {}, shape: {}\".format(key, val.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G5co4M9uSnrk"},"source":["# Create the Transformer\n","\n","The Transformer consists of:\n","* An encoder\n","* A decoder\n","* A final linear layer \n","\n","The overall workflow is:\n","* The encoder takes Portuguese sequence as the input. \n","* The decoder takes the encoder's output and the English sequence as the input. After attention's layers, the decoder uses the linear and softmax to get the subword indices."]},{"cell_type":"code","metadata":{"id":"KFOPzCvkahJW","executionInfo":{"status":"aborted","timestamp":1648789652779,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, \n","               input_vocab_size, target_vocab_size, \n","               input_pos_enc, target_pos_enc, dropout_rate=0.1):\n","    \"\"\"\n","    Args:\n","      num_layers: the number of layers stacked in the encoder and the decoder\n","      d_model: the dimension of representation used in the encoder and the decoder\n","      num_heads: how many splited heads\n","      dff: the dimension of point-wise feed forard network used in the encoder an the decoder\n","      input_vocab_size: for creating an embedding layer in the encoder\n","      target_vocab_size: for creating an embedding layer in the decoder\n","      input_pos_enc: the max position for encoding\n","      target_pos_enc: tha max position for decoding\n","      dropout_rate: the rate for the Dropout layer\n","    \"\"\"\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers=num_layers, \n","                           d_model=d_model, \n","                           num_heads=num_heads, \n","                           dff=dff, \n","                           input_vocab_size=input_vocab_size, \n","                           max_positional_encoding=input_pos_enc,\n","                           dropout_rate=dropout_rate)\n","    self.decoder = Decoder(num_layers=num_layers, \n","                           d_model=d_model, \n","                           num_heads=num_heads, \n","                           dff=dff, \n","                           target_vocab_size=target_vocab_size, \n","                           max_positional_encoding=target_pos_enc,\n","                           dropout_rate=dropout_rate)\n","    # the final layer generates the subword probabilities of the target language\n","    self.final_layer = tf.keras.layers.Dense(units=target_vocab_size)\n","\n","  def create_masks(self, inp, tar):\n","    \"\"\"Create the masks.\"\"\"\n","    # Encoder padding mask\n","    enc_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by\n","    # the decoder.\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)  \n","    return enc_padding_mask, look_ahead_mask, dec_padding_mask  \n","\n","  def call(self, inputs, targets, training):\n","    \"\"\"\n","    Args:\n","      inputs: the subword list created from the Portuguese text\n","      targets: the subword list created from the English text\n","      training: True or False\n","\n","    Returns:\n","      final_output: (batch_size, target_seq_length, target_vocab_size)\n","      attentions: a dictionary keeps the attentions of blocks in each decoder layer\n","    \"\"\"\n","\n","    \"\"\"\n","    enc_padding_mask: the padded mask used in the encoder\n","    look_ahead_mask: the look-ahead mask used in the \n","                      button multi-head attenation of the decoder\n","    dec_padding_mask: the padded mask used in the \n","                      upper multi-head attention of the decoder    \n","    \"\"\"\n","    enc_padding_mask, look_ahead_mask, dec_padding_mask \\\n","      = self.create_masks(inputs, targets)\n","    print(inputs)\n","    print(targets)\n","\n","    # enc_output: (batch_size, inputs_seq_length, d_model)\n","    enc_output = self.encoder(inputs, training=training, mask=enc_padding_mask) \n","\n","    # dec_output: (batch_size, target_seq_length, d_model)\n","    dec_output, attentions = self.decoder(targets, \n","                                          encoder_output=enc_output, \n","                                          training=training, \n","                                          look_ahead_mask=look_ahead_mask, \n","                                          mask=dec_padding_mask)\n","\n","    # final_output: (batch_size, target_seq_length, target_vocab_size)\n","    final_output = self.final_layer(dec_output)\n","\n","    return final_output, attentions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's build a simple transformer."],"metadata":{"id":"N3PfDSGQR3sN"}},{"cell_type":"code","metadata":{"id":"tYqIMcOqmuL6","executionInfo":{"status":"aborted","timestamp":1648789652780,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["simple_transformer = Transformer(num_layers=2, \n","                                 d_model=512, \n","                                 num_heads=8, \n","                                 dff=2048, \n","                                 input_vocab_size=len(pt_vocab), \n","                                 target_vocab_size=len(en_vocab),\n","                                 input_pos_enc=10000, \n","                                 target_pos_enc=10000)\n","\n","temp_input = tf.random.uniform(shape=(64, 70), minval=0, maxval=200, dtype=tf.int32)\n","temp_target = tf.random.uniform(shape=(64, 68), minval=0, maxval=200, dtype=tf.int32)\n","\n","final_output, attentions = simple_transformer(\n","    inputs=temp_input, \n","    targets=temp_target, \n","    training=False)\n","\n","print(\"The shape of the decoder: {}\".format(final_output.shape))\n","for key, val in attentions.items():\n","  print(\"Key: {}, Val shape: {}\".format(key, val.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rakPMFfO2Qe3"},"source":["# Set the Hyperparameters"]},{"cell_type":"code","metadata":{"id":"kzlPkqB-oAKO","executionInfo":{"status":"aborted","timestamp":1648789652780,"user_tz":-480,"elapsed":28,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = len(pt_vocab)\n","target_vocab_size = len(en_vocab)\n","dropout_rate = 0.1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ji1m_dpw3FUE"},"source":["# Customized Optimizer\n","\n","Use the Adam optimizer with $\\beta_1 = 0.9$, $\\beta_2 = 0.98$, and $\\epsilon=10^{-9}$. The learning rate scheduler follows the formula.\n","\n","$$lrate = d^{-0.5}_{model} * min(step\\_num^{-0.5}, step\\_num * warmup\\_steps^{-1.5})$$"]},{"cell_type":"code","metadata":{"id":"GqiFIj_h2x4o","executionInfo":{"status":"aborted","timestamp":1648789652781,"user_tz":-480,"elapsed":29,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  \"\"\"Define a customized optimizer scheduler.\"\"\"\n","  def __init__(self, d_model, warmup_step=4000):\n","    self.d_model = tf.cast(d_model, tf.float32)\n","    self.warmup_step = warmup_step\n","  \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_step ** (-1.5))\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbVReoi56zq3","executionInfo":{"status":"aborted","timestamp":1648789652781,"user_tz":-480,"elapsed":29,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["learning_rate = CustomSchedule(d_model=d_model, warmup_step=4000)\n","optimizer = tf.keras.optimizers.Adam(\n","    learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9MWHbiT8Iy6","executionInfo":{"status":"aborted","timestamp":1648789652781,"user_tz":-480,"elapsed":29,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["simple_lr = CustomSchedule(d_model=d_model)\n","\n","plt.plot(simple_lr(tf.range(40000, dtype=tf.float32)))\n","plt.xlabel(\"Training Step\")\n","plt.ylabel(\"Learning Rate\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EKr__xld9Zzw"},"source":["# Loss and Metrics"]},{"cell_type":"code","metadata":{"id":"gAM-6s5o8eco","executionInfo":{"status":"aborted","timestamp":1648789652782,"user_tz":-480,"elapsed":30,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","  from_logits=True, reduction='none'\n",")\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","\n","  # real: (batch_size, seq_length)\n","  # pred: (batch_size, seq_length, target_vocab_size)\n","  loss = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss.dtype)\n","  loss *= mask\n","  return tf.reduce_mean(loss)\n","\n","def accuracy_function(real, pred):\n","  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n","\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  accuracies = tf.math.logical_and(mask, accuracies)\n","\n","  accuracies = tf.cast(accuracies, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eWaL6QJoARbg","executionInfo":{"status":"aborted","timestamp":1648789652782,"user_tz":-480,"elapsed":29,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["#train_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='train_loss')\n","train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kg9ZLGWyAvpP"},"source":["# Training and Checkpointing"]},{"cell_type":"code","metadata":{"id":"Q9e01tDYAsX1","executionInfo":{"status":"aborted","timestamp":1648789652782,"user_tz":-480,"elapsed":29,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["transformer = Transformer(num_layers=num_layers, \n","                          d_model=d_model, \n","                          num_heads=num_heads, \n","                          dff=dff, \n","                          input_vocab_size=len(pt_vocab), \n","                          target_vocab_size=len(en_vocab),\n","                          input_pos_enc=10000, \n","                          target_pos_enc=10000,\n","                          dropout_rate=dropout_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QK7DnW9Ad17g"},"source":["Create the checkpoint and its manager to save it every `n` epochs."]},{"cell_type":"code","metadata":{"id":"buY-71MlPGYt","executionInfo":{"status":"aborted","timestamp":1648789653148,"user_tz":-480,"elapsed":394,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["# saving_path = \"/content/gdrive/My Drive/tmp/model_saving/\"\n","saving_path = \"./\"\n","assert os.path.exists(saving_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GO-bCSMHg00A","executionInfo":{"status":"aborted","timestamp":1648789653148,"user_tz":-480,"elapsed":394,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["ckpt_path = os.path.join(saving_path, \"ckpts\", \"train\")\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer, \n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(checkpoint=ckpt, \n","                                          directory=ckpt_path, \n","                                          max_to_keep=5)\n","\n","# restore the checkpoint if it exists\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print(\"Latest checkpoint restored.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VyDWxV2viWov"},"source":["The target is divided into `target_input` and `target_real`. The `target_input` is passed as the input to the decoder. The `target_real` is the same input shifted by 1 to represent next token that should be predicted.\n","\n","For example, \n","* The `target` string is `SOS Transformer is awesome EOS`.\n","* The `target_input` string is `SOS Transformer is awesome`.\n","* The `target_real` string is `Transformer is awesome EOS`.\n","\n","Train the transformer using `teacher-forcing`. The teacher-forcing is to pass the correct token to the next step regardless of what the output model predicts.\n","\n","As the transformer predicts each word, it is allowed to look at the previous words of the input sequence to make the prediction. However, it is not allowed to peak at the expected output token using a `look-ahead mask`.\n"]},{"cell_type":"code","metadata":{"id":"nZCO1wzVh6ac","executionInfo":{"status":"aborted","timestamp":1648789653149,"user_tz":-480,"elapsed":395,"user":{"displayName":"王DevOps","userId":"04300517850278510646"}}},"source":["train_step_signature = [\n","  tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # input shape: (batch_size, seq_len)\n","  tf.TensorSpec(shape=(None, None), dtype=tf.int64),  # target shape: (batch_size, seq_len)\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inputs, targets):\n","  targets_input = targets[:, :-1]\n","  targets_real = targets[:, 1:]\n","  \n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inputs=inputs, \n","                                 targets=targets_input,\n","                                 training=True)\n","    \n","    loss = loss_function(targets_real, predictions)\n","\n","  # calculate the gradients and apply them to trainable variables\n","  gradients = tape.gradient(loss, transformer.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","  # update the loss and accuracy of the batch data\n","  train_loss(loss)\n","  train_accuracy(targets_real, predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TblrpfYuQak"},"source":["Complete the training process."]},{"cell_type":"code","metadata":{"id":"oh51bZcRuPe0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"639d9881-4f4c-4cd3-fb4c-b4c775c678d2"},"source":["EPOCHS = 20\n","peroid = 0\n","\n","for epoch in range(EPOCHS):\n","  peroid = time.time()\n","\n","  # reset the statue\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  # inputs: Portuguese, targets: English\n","  for batch, (inputs, targets) in enumerate(train_dataset):\n","    train_step(inputs, targets)\n","\n","    if (batch + 1) % 200 == 0:\n","      print(\"Epoch {}, Batch {}, Loss {:.6f}, Accuracy {:.6f}\".format(\n","        epoch + 1, batch, train_loss.result(), train_accuracy.result()\n","      ))\n","\n","  # save the model  \n","  if (epoch + 1) % 5 == 0:\n","    ckpt_manager.save()\n","    print(\"Save the checkpoint at epoch {}.\".format(epoch + 1))\n","\n","  # output the training status every epoch\n","  print(\"Epoch {}, Loss {:.4f}, Accuracy {:.4f}\".format(\n","    epoch + 1, train_loss.result(), train_accuracy.result()\n","  ))\n","\n","  print(\"Epoch took {} seconds.\\n\".format(time.time() - peroid))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"inputs:0\", shape=(None, None), dtype=int64)\n","Tensor(\"strided_slice:0\", shape=(None, None), dtype=int64)\n","Tensor(\"inputs:0\", shape=(None, None), dtype=int64)\n","Tensor(\"strided_slice:0\", shape=(None, None), dtype=int64)\n","Epoch 1, Batch 0, Loss 4.350060, Accuracy 0.000000\n","Epoch 1, Batch 200, Loss 3.907653, Accuracy 0.020248\n","Epoch 1, Batch 400, Loss 3.569590, Accuracy 0.027303\n","Epoch 1, Batch 600, Loss 3.301102, Accuracy 0.042865\n","Epoch 1, Loss 3.1955, Accuracy 0.0497\n","Epoch took 1116.0849742889404 seconds.\n","\n","Epoch 2, Batch 0, Loss 2.530260, Accuracy 0.105469\n","Epoch 2, Batch 200, Loss 2.399493, Accuracy 0.103517\n","Epoch 2, Batch 400, Loss 2.358044, Accuracy 0.110875\n","Epoch 2, Batch 600, Loss 2.314605, Accuracy 0.115790\n","Epoch 2, Loss 2.2988, Accuracy 0.1174\n","Epoch took 1062.0612633228302 seconds.\n","\n","Epoch 3, Batch 0, Loss 1.968953, Accuracy 0.123397\n","Epoch 3, Batch 200, Loss 2.100905, Accuracy 0.133483\n"]}]},{"cell_type":"markdown","metadata":{"id":"SLnT7fPWlRn_"},"source":["# Evaluate\n","\n","The following steps are used in the evaluation.\n","* Encode the input string using Portuguese tokenizer. Add the start token (`tokenizer_pt.vocabz_size`) and the end token (`tokenizer_pt.vocab_size + 1`) to the encoded string at the head and the end of the string.\n","* The decoder input is only the start token (`tokenizer_en.vocab_size`).\n","* Calculate the `look_ahead_mask` and the `padding_mask`.\n","* The `decoder` then outputs the predictions by looking at the encoder output and its own output.\n","* `Concatenate` the predicted token to the decoder input and pass it to the decoder for the next token."]},{"cell_type":"code","metadata":{"id":"G8bWJBDZJy0v"},"source":["def evaluate(inputs):\n","  \"\"\"\n","  Args:\n","    inputs shape: a sentence in string\n","  \"\"\"\n","\n","  # encoder\n","  encoder_start = [pt_vocab[2]]\n","  encoder_end = [pt_vocab[3]]\n","  encoder_inputs = encoder_start + tokenizer_pt.encode(inputs) + encoder_end\n","  encoder_inputs = tf.expand_dims(encoder_inputs, axis=0)    # (batch_size == 1, encoder_list_len)\n","\n","  # decoder\n","  outputs = [en_vocab[2]]\n","  outputs = tf.expand_dims(outputs, axis=0)  # (batch_size == 1, 1)\n","\n","  # continue predicting\n","  for i in range(MAX_LENGTH):\n","    # get the final output\n","    # output shape: (batch_size == 1, outputs_len, target_vocab_size)\n","    out, attns = transformer(encoder_inputs, \n","                             outputs, \n","                             False)\n","    \n","    # output_token shape: (batch_size, 1) => (batch_size, 1)\n","    output_token = tf.cast(tf.argmax(out[:, -1:, :], axis=-1), tf.int32)\n","\n","    # check whether the prediction token is the end token\n","    if output_token == 3:\n","      return tf.squeeze(outputs, axis=0), attns\n","\n","    outputs = tf.concat([outputs, output_token], axis=-1)\n","\n","  return tf.squeeze(outputs, axis=0), attns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kapwmY43y8sV"},"source":["def plot_attention_weights(attention, sentence, result, layer):\n","  \"\"\"\n","  Args:\n","    attention shape: (batch_size, num_heads, None, None)\n","  \"\"\"\n","  fig = plt.figure(figsize=(16, 8))\n","\n","  sentence = tokenizer_pt.tokenize(sentence)\n","  # remove the axis of the batch size\n","  attention = tf.squeeze(attention[layer], axis=0)\n","\n","  for head in range(attention.shape[0]):\n","    ax = fig.add_subplot(2, 4, head+1)\n","\n","    # plot the attention\n","    ax.matshow(attention[head][:-1, :], cmap=\"viridis\")\n","\n","    fontdicts = {'fontsize': 10}\n","\n","    ax.set_xticks(range(len(sentence) + 2))  # inputs\n","    ax.set_yticks(range(len(result)))  # targets\n","\n","    ax.set_ylim(len(result) - 1.5, 0.5)\n","\n","    # set labels\n","    ax.set_xticklabels(\n","      [\"<start>\"] + [tokenizer_pt.detokenize([t]) for t in sentence] + [\"<end>\"],\n","      fontdict=fontdicts,\n","      rotation=90\n","    )\n","\n","    ax.set_yticklabels(\n","      [tokenizer_en.tokenize([t]) for t in result if t < len(en_vocab)],\n","      fontdict=fontdicts\n","    )\n","\n","    ax.set_xlabel('head {}'.format(head + 1))\n","\n","  plt.tight_layout()\n","  plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wNqiBze6IiG"},"source":["def translate(sentence, plot=''):\n","  result, attns = evaluate(sentence)\n","  predicted_sentence = tokenizer_en.detokenize([t for t in result if t < len(en_vocab)])\n","\n","  print(\"Input: {}\".format(sentence))\n","  print(\"Predicted: {}\".format(predicted_sentence))\n","\n","  if plot:\n","    plot_attention_weights(attns, sentence=sentence, result=result, layer=\"decoder_layer2_block2\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMhYJXhy6mn9","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"ef6729d5-35a4-454a-98d3-94af90f359c6","executionInfo":{"status":"ok","timestamp":1583986159130,"user_tz":-480,"elapsed":2828,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["translate(\"este é um problema que temos que resolver.\")\n","print (\"Real translation: this is a problem we have to solve .\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: este é um problema que temos que resolver.\n","Predicted: this is a problem that we have to solve it , and this is a problem .\n","Real translation: this is a problem we have to solve .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zdkh45-m7y07","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d974d204-743b-449f-f869-2a1fd821a650","executionInfo":{"status":"ok","timestamp":1583986203398,"user_tz":-480,"elapsed":2494,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n","print (\"Real translation: and my neighboring homes heard about this idea .\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: os meus vizinhos ouviram sobre esta ideia.\n","Predicted: my neighbors heard about this idea .\n","Real translation: and my neighboring homes heard about this idea .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bjeLYgK272yi","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"d5f53cdb-07ff-4e1a-9488-2c5ba1190242","executionInfo":{"status":"ok","timestamp":1583986214052,"user_tz":-480,"elapsed":4590,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n","print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n","Predicted: so i 'm going to really close with you some of a few magical things that happened .\n","Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oP0VfZMC6sYx","colab":{"base_uri":"https://localhost:8080/","height":656},"outputId":"cd2854d2-bc79-4d4e-d4b8-4b0c8e6b85a2","executionInfo":{"status":"ok","timestamp":1583986161612,"user_tz":-480,"elapsed":3788,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgXVAgB9A4OE1KXeAfp5b-xUS2OSsbqSRVEe_UETw=s64","userId":"04300517850278510646"}}},"source":["translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n","print (\"Real translation: this is the first book i've ever done.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input: este é o primeiro livro que eu fiz.\n","Predicted: this is the first book that i did in my own life .\n","Real translation: this is the first book i've ever done.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABEAAAAI4CAYAAABqXpzZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 1152x576 with 8 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"ouYns2hF7eNe"},"source":[""],"execution_count":null,"outputs":[]}]}