{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enviro: macos, docker\n",
    "envir = \"macos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "db = None\n",
    "\n",
    "tags_to_index = {}   # {word1: 1, word2: 2, ...}\n",
    "index_to_tags = {}   # {1: word1, 2: word2, ...}\n",
    "train_dataset_raw = {}\n",
    "train_dataset = []\n",
    "test_dataset_raw = {}\n",
    "test_dataset = []\n",
    "dataset_vocab = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use a word2vec data structure with 300 dimensions from google news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/19/8ecba86351de0eacb9baf1cc49ba86315cd91bc672acd74d6e4e709eb482/gensim-3.6.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (24.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 24.0MB 818kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.14.3)\n",
      "Collecting smart-open>=1.2.1 (from gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/1f/6f27e3682124de63ac97a0a5876da6186de6c19410feab66c1543afab055/smart_open-1.7.1.tar.gz\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied: boto>=2.32 in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
      "Collecting bz2file (from smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/39/122222b5e85cd41c391b68a99ee296584b2a2d1d233e7ee32b4532384f2d/bz2file-0.98.tar.gz\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Collecting boto3 (from smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/af/ff24b42daacdc929629f4f85ce8a54ee1c6591475b5067d180028feffb57/boto3-1.9.28-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K    100% |████████████████████████████████| 133kB 26.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim) (2018.8.24)\n",
      "Collecting botocore<1.13.0,>=1.12.28 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/12/c3d6a94626c5cfd829d8ad88e333f70b2a17636b1b20daa17c48839125bb/botocore-1.12.28-py2.py3-none-any.whl (4.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.7MB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.2.0,>=0.1.10 (from boto3->smart-open>=1.2.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 12.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.2.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.28->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda3/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.28->boto3->smart-open>=1.2.1->gensim) (2.7.3)\n",
      "Building wheels for collected packages: smart-open, bz2file\n",
      "  Running setup.py bdist_wheel for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jiankaiwang/Library/Caches/pip/wheels/23/00/44/e5b939f7a80c04e32297dbd6d96fa3065af89ecf57e2b5f89f\n",
      "  Running setup.py bdist_wheel for bz2file ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/jiankaiwang/Library/Caches/pip/wheels/81/75/d6/e1317bf09bf1af5a30befc2a007869fa6e1f516b8f7c591cb9\n",
      "Successfully built smart-open bz2file\n",
      "Installing collected packages: bz2file, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.9.28 botocore-1.12.28 bz2file-0.98 gensim-3.6.0 jmespath-0.9.3 s3transfer-0.1.13 smart-open-1.7.1\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if envir == \"docker\":\n",
    "    datastore = os.path.join(\"/\",\"notebooks\",\"data\",\"googlenews\",\"GoogleNews-vectors-negative300.bin\")\n",
    "elif envir == \"macos\":\n",
    "    datastore = os.path.join(\"/\",\"Volumes\",\"Data\",\"googlenews\",\"GoogleNews-vectors-negative300.bin\")\n",
    "\n",
    "if not os.path.exists(datastore):\n",
    "    raise IOError(\"Not such file {}.\".format(datastore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if envir == \"docker\":\n",
    "    leveldb_path = os.path.join(\"/\",\"notebooks\",\"devops\",\"tmp\",\"word2vecdb\")\n",
    "elif envir == \"macos\":\n",
    "    leveldb_path = \"/Users/jiankaiwang/devops/tmp/word2vecdb\"\n",
    "\n",
    "if envir == \"docker\":\n",
    "    pre_data_path = os.path.join(\"/\",\"notebooks\",\"data\",\"CoNLL-2000\")\n",
    "elif envir == \"macos\":\n",
    "    pre_data_path = os.path.join(\"/\",\"Volumes\",\"Data\",\"CoNLL-2000\")   \n",
    "\n",
    "origin_train = os.path.join(pre_data_path, \"train.txt\")\n",
    "origin_test = os.path.join(pre_data_path, \"test.txt\")\n",
    "\n",
    "preprocess_train = os.path.join(pre_data_path, \"pp_train.txt\")\n",
    "preprocess_test = os.path.join(pre_data_path, \"pp_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessWord(dataList, training_data=True):\n",
    "    \"\"\"\n",
    "    parse train and test origin data\n",
    "    \"\"\"\n",
    "    global train_dataset, test_dataset\n",
    "    \n",
    "    ref_dataset = dataList\n",
    "    tmp_dataset = train_dataset if training_data else test_dataset\n",
    "    \n",
    "    count = 0\n",
    "    while count < len(dataList):\n",
    "        pair = ref_dataset[count]\n",
    "        if count < len(ref_dataset) - 1:\n",
    "            next_pair = ref_dataset[count + 1]\n",
    "            \n",
    "            # solve New NN\n",
    "            #       York NN\n",
    "            # pair[1] == next_pair[1] : the same property\n",
    "            if (pair[0] + \"_\" + next_pair[0]) in model and pair[1] == next_pair[1]:\n",
    "                tmp_dataset.append([pair[0] + \"_\" + next_pair[0], pair[1]])\n",
    "                count += 2\n",
    "                continue\n",
    "                \n",
    "        # number replacement\n",
    "        word = re.sub(\"\\d\", \"#\", pair[0])\n",
    "        \n",
    "        # replace '-' with _\n",
    "        word = re.sub(\"-\", \"_\", word)\n",
    "        \n",
    "        # only preserve the word we needed\n",
    "        if word in model:\n",
    "            tmp_dataset.append([word, pair[1]])\n",
    "            count += 1\n",
    "            continue\n",
    "        \n",
    "        # handle both \"_word\" or \"word_\"\n",
    "        if \"_\" in word:\n",
    "            subwords = word.split(\"_\")\n",
    "            for sw in subwords:\n",
    "                if not (sw.isspace() or len(sw) == 0):\n",
    "                    tmp_dataset.append([sw, pair[1]])\n",
    "            count += 1\n",
    "            continue\n",
    "            \n",
    "        train_dataset.append([word, pair[1]])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the whole google news words\n",
    "model = KeyedVectors.load_word2vec_format(datastore, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.39550781e-02  1.58203125e-01  1.04003906e-01 -2.89916992e-04\n",
      "  5.32226562e-02 -8.64257812e-02 -1.98242188e-01 -7.47070312e-02\n",
      "  2.89062500e-01  1.11816406e-01 -1.86523438e-01 -9.52148438e-02\n",
      "  1.77734375e-01 -1.62109375e-01 -3.56445312e-02  3.68652344e-02\n",
      "  3.49426270e-03 -5.15136719e-02 -1.55273438e-01 -3.18359375e-01\n",
      " -8.97216797e-03  1.89453125e-01 -6.78710938e-02 -9.47265625e-02\n",
      "  1.12304688e-01 -1.09375000e-01 -7.87353516e-03  2.32421875e-01\n",
      " -2.38281250e-01  1.64062500e-01  1.62109375e-01 -7.32421875e-02\n",
      " -1.41601562e-01 -2.29492188e-01 -1.68945312e-01  1.73828125e-01\n",
      " -2.27539062e-01 -1.25000000e-01  2.11914062e-01  2.63671875e-01\n",
      "  3.18359375e-01  1.13769531e-01 -1.28906250e-01  2.59765625e-01\n",
      "  1.91406250e-01  6.68945312e-02  1.03027344e-01  1.75781250e-01\n",
      " -4.12597656e-02  4.54101562e-02  1.80664062e-01 -3.49121094e-02\n",
      "  1.93359375e-01  1.00708008e-02  5.32226562e-02  1.19140625e-01\n",
      " -1.21093750e-01 -8.10546875e-02  1.33789062e-01 -1.74804688e-01\n",
      "  4.07714844e-02  5.32226562e-02  2.28271484e-02 -2.23632812e-01\n",
      " -2.20703125e-01  1.59179688e-01  2.11181641e-02  7.12890625e-02\n",
      " -3.36914062e-02  3.34472656e-02  1.02539062e-01 -6.29882812e-02\n",
      "  3.53515625e-01  2.24609375e-01 -2.38281250e-01  2.58789062e-02\n",
      " -1.89208984e-02  8.44726562e-02  1.65039062e-01 -1.79687500e-01\n",
      " -5.29785156e-02 -1.56250000e-01 -1.89453125e-01 -1.33789062e-01\n",
      "  2.48046875e-01 -1.16210938e-01 -1.84570312e-01  5.85937500e-02\n",
      " -2.67333984e-02  1.47460938e-01 -3.68652344e-02  2.33398438e-01\n",
      " -6.03027344e-02 -2.35351562e-01 -5.63964844e-02 -1.24511719e-01\n",
      "  1.87500000e-01  1.44531250e-01 -1.69677734e-02 -8.69140625e-02\n",
      " -8.39843750e-02  4.51660156e-02 -1.47460938e-01  6.68945312e-02\n",
      "  1.25976562e-01  8.30078125e-03  1.55273438e-01 -6.25000000e-02\n",
      " -4.22363281e-02 -6.83593750e-02  3.35937500e-01  1.28906250e-01\n",
      " -1.80664062e-01 -2.01416016e-02  1.44531250e-01  2.23632812e-01\n",
      " -1.13769531e-01 -7.12890625e-02 -2.57812500e-01  4.61425781e-02\n",
      " -1.83105469e-02  1.29882812e-01  3.33984375e-01  2.19726562e-01\n",
      "  3.56445312e-02  1.98364258e-03 -1.58691406e-02 -1.07910156e-01\n",
      " -2.76184082e-03  1.25000000e-01 -1.58203125e-01 -5.32226562e-02\n",
      "  1.03027344e-01 -2.01171875e-01  3.49121094e-02  1.72851562e-01\n",
      " -1.17675781e-01 -1.90429688e-02 -1.51367188e-01  2.25830078e-02\n",
      " -6.93359375e-02  3.46679688e-02  1.43554688e-01  1.25000000e-01\n",
      " -1.35742188e-01 -1.11816406e-01  1.16699219e-01 -1.10839844e-01\n",
      " -4.27246094e-02  2.85156250e-01 -3.03955078e-02 -1.87988281e-02\n",
      " -1.75781250e-01  8.25195312e-02 -9.22851562e-02  5.34667969e-02\n",
      " -6.12792969e-02  1.43554688e-01  6.73828125e-02 -1.78710938e-01\n",
      " -3.63281250e-01  1.45507812e-01  1.75781250e-01 -3.75976562e-02\n",
      " -1.19628906e-01  4.17480469e-02 -1.62109375e-01 -2.04101562e-01\n",
      "  2.79541016e-02  1.01074219e-01  4.61425781e-02 -1.58203125e-01\n",
      "  1.58203125e-01 -3.02734375e-01 -6.44531250e-02 -3.76953125e-01\n",
      "  5.34057617e-03 -2.32421875e-01 -1.18652344e-01 -9.17968750e-02\n",
      " -1.12792969e-01  4.76074219e-02  7.66601562e-02  3.85742188e-02\n",
      " -2.04101562e-01 -1.29882812e-01 -4.16015625e-01 -4.00390625e-02\n",
      " -1.08398438e-01  1.72851562e-01  1.63085938e-01 -1.05957031e-01\n",
      "  6.44531250e-02  1.47460938e-01 -2.16796875e-01  5.02929688e-02\n",
      "  4.73632812e-02  2.04101562e-01 -2.27539062e-01 -1.24023438e-01\n",
      " -3.19824219e-02  2.26562500e-01  8.00781250e-02 -4.17480469e-02\n",
      "  1.25976562e-01 -1.93359375e-01 -2.24609375e-01 -1.41601562e-02\n",
      " -9.17968750e-02  3.93066406e-02 -2.91015625e-01 -8.78906250e-03\n",
      "  1.81640625e-01 -1.94091797e-02 -1.52343750e-01  5.39550781e-02\n",
      "  1.69921875e-01  1.70898438e-02 -9.42382812e-02 -9.86328125e-02\n",
      " -5.00488281e-02  2.63671875e-01 -7.81250000e-02 -8.00781250e-02\n",
      "  5.83496094e-02 -1.35742188e-01 -7.95898438e-02 -1.22070312e-01\n",
      "  3.71093750e-02  1.23901367e-02 -1.46484375e-01  8.98437500e-02\n",
      " -2.61718750e-01  2.00195312e-02 -3.12500000e-01  1.76757812e-01\n",
      "  1.25000000e-01 -5.22460938e-02  1.53320312e-01  1.80664062e-02\n",
      "  5.71289062e-02 -5.68847656e-02  9.76562500e-03 -1.64062500e-01\n",
      " -1.02539062e-01  1.88476562e-01 -1.34277344e-02  1.64062500e-01\n",
      " -1.28173828e-02 -1.68945312e-01 -1.28906250e-01 -1.08886719e-01\n",
      " -2.61718750e-01  4.29687500e-02 -7.47070312e-02  6.12792969e-02\n",
      "  2.50000000e-01  6.22558594e-02 -1.25122070e-02 -2.99072266e-02\n",
      "  3.04687500e-01  1.34765625e-01 -5.05371094e-02  1.35742188e-01\n",
      "  1.67968750e-01  1.10839844e-01 -1.12304688e-01  8.44726562e-02\n",
      "  4.07714844e-02 -1.50390625e-01  1.11816406e-01  7.56835938e-02\n",
      "  1.82617188e-01  2.08984375e-01 -2.06054688e-01 -1.75781250e-02\n",
      " -8.10546875e-02 -2.81250000e-01  1.42578125e-01 -2.71484375e-01\n",
      "  1.23535156e-01  7.91015625e-02 -9.22851562e-02  5.24902344e-02\n",
      " -1.98242188e-01 -1.90429688e-01 -6.39648438e-02  1.82617188e-01\n",
      "  1.55273438e-01  1.58691406e-02  8.15429688e-02  1.41601562e-01\n",
      " -2.09960938e-01  1.19140625e-01 -3.41796875e-02 -8.69140625e-02\n",
      "  2.80761719e-02 -7.76367188e-02 -1.36718750e-01 -9.13085938e-02]\n"
     ]
    }
   ],
   "source": [
    "# a word2vec example from google news\n",
    "print(model['record'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove preprocessed train data.\n",
      "Remove preprocessed test data.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(preprocess_train): \n",
    "    os.remove(preprocess_train)\n",
    "    print(\"Remove preprocessed train data.\")\n",
    "\n",
    "if os.path.exists(preprocess_test):\n",
    "    os.remove(preprocess_test)\n",
    "    print(\"Remove preprocessed test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train size: 211727\n",
      "['Confidence', 'NN', 'B-NP']\n"
     ]
    }
   ],
   "source": [
    "# read origin train data\n",
    "with open(origin_train, \"r\") as f:\n",
    "    train_dataset_raw = f.readlines()\n",
    "    train_dataset_raw = [element.split() for element in train_dataset_raw if len(element.split()) > 0]\n",
    "print(\"Total Train size: {}\".format(len(train_dataset_raw)))\n",
    "\n",
    "# show data content\n",
    "print(train_dataset_raw[0])\n",
    "\n",
    "# preprocessing origin train data\n",
    "preprocessWord(dataList=train_dataset_raw, training_data=True)\n",
    "\n",
    "# write out the prepcrossed train data\n",
    "with open(preprocess_train, \"w\") as fout:\n",
    "    for pair in train_dataset:\n",
    "        fout.write(\"{} {}\\n\".format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Test size: 47377\n",
      "['Rockwell', 'NNP', 'B-NP']\n"
     ]
    }
   ],
   "source": [
    "# read origin test data\n",
    "with open(origin_test, \"r\") as f:\n",
    "    test_dataset_raw = f.readlines()\n",
    "    test_dataset_raw = [element.split() for element in test_dataset_raw if len(element.split()) > 0]\n",
    "print(\"Total Test size: {}\".format(len(test_dataset_raw)))\n",
    "\n",
    "# show data content\n",
    "print(test_dataset_raw[0])\n",
    "\n",
    "# preprocessing origin train data\n",
    "preprocessWord(dataList=test_dataset_raw, training_data=False)\n",
    "\n",
    "# write out the prepcrossed train data\n",
    "with open(preprocess_test, \"w\") as fout:\n",
    "    for pair in test_dataset:\n",
    "        fout.write(\"{} {}\\n\".format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create two mapping files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for pair in train_dataset + test_dataset:\n",
    "    dataset_vocab[pair[0]] = 1\n",
    "    \n",
    "    if pair[1] not in tags_to_index:\n",
    "        tags_to_index[pair[1]] = count\n",
    "        index_to_tags[count] = pair[1]\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18485\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NN': 0, 'IN': 1, 'DT': 2, 'VBZ': 3, 'RB': 4, 'VBN': 5, 'TO': 6, 'VB': 7, 'JJ': 8, 'NNS': 9, 'NNP': 10, ',': 11, 'CC': 12, 'POS': 13, '.': 14, 'VBP': 15, 'VBG': 16, 'PRP$': 17, 'CD': 18, '``': 19, \"''\": 20, 'VBD': 21, 'EX': 22, 'MD': 23, '#': 24, '(': 25, '$': 26, ')': 27, 'NNPS': 28, 'PRP': 29, 'JJS': 30, 'WP': 31, 'RBR': 32, 'JJR': 33, 'WDT': 34, 'WRB': 35, 'RBS': 36, 'PDT': 37, 'RP': 38, ':': 39, 'FW': 40, 'WP$': 41, 'SYM': 42, 'UH': 43}\n"
     ]
    }
   ],
   "source": [
    "print(tags_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'NN', 1: 'IN', 2: 'DT', 3: 'VBZ', 4: 'RB', 5: 'VBN', 6: 'TO', 7: 'VB', 8: 'JJ', 9: 'NNS', 10: 'NNP', 11: ',', 12: 'CC', 13: 'POS', 14: '.', 15: 'VBP', 16: 'VBG', 17: 'PRP$', 18: 'CD', 19: '``', 20: \"''\", 21: 'VBD', 22: 'EX', 23: 'MD', 24: '#', 25: '(', 26: '$', 27: ')', 28: 'NNPS', 29: 'PRP', 30: 'JJS', 31: 'WP', 32: 'RBR', 33: 'JJR', 34: 'WDT', 35: 'WRB', 36: 'RBS', 37: 'PDT', 38: 'RP', 39: ':', 40: 'FW', 41: 'WP$', 42: 'SYM', 43: 'UH'}\n"
     ]
    }
   ],
   "source": [
    "print(index_to_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save into leveldb / pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add into cache to\n",
      "add into cache ,\n",
      "add into cache a\n",
      "add into cache and\n",
      "add into cache 's\n",
      "add into cache .\n",
      "add into cache of\n",
      "add into cache ``\n",
      "add into cache ''\n",
      "add into cache L.P\n",
      "add into cache '\n",
      "add into cache hotel\\/casino\n",
      "add into cache ;\n",
      "add into cache :\n",
      "add into cache Underseas\n",
      "add into cache ?\n",
      "Inserted 1000 words out of 18485 total.\n",
      "add into cache Ohbayashi\n",
      "add into cache establshed\n",
      "add into cache B.A.T\n",
      "add into cache Zoete\n",
      "add into cache Noxell\n",
      "add into cache P&G\n",
      "add into cache Boelkow\n",
      "add into cache G.m.b\n",
      "add into cache Fleet\\/Norstar\n",
      "add into cache I.E.P.\n",
      "Inserted 2000 words out of 18485 total.\n",
      "add into cache SKr#.#\n",
      "add into cache Bfree\n",
      "add into cache Herslow\n",
      "add into cache Kurtanjek\n",
      "add into cache SE\\/##\n",
      "add into cache IIcx\n",
      "add into cache ...\n",
      "add into cache Polymerix\n",
      "add into cache Polycast\n",
      "add into cache Kushkin\n",
      "add into cache R.I\n",
      "add into cache Ebasco\n",
      "add into cache Enserch\n",
      "add into cache Asia\\/Australia\n",
      "add into cache Infotechnology\n",
      "add into cache Webster\\/Eagle\n",
      "add into cache #\\/#\n",
      "Inserted 3000 words out of 18485 total.\n",
      "add into cache Qintex\n",
      "add into cache MGM\\/UA\n",
      "add into cache Inmac\n",
      "add into cache Pharmics\n",
      "add into cache N.Y\n",
      "add into cache mioxidil\n",
      "add into cache chlorazepate\n",
      "add into cache dipotassium\n",
      "add into cache meclofenamate\n",
      "add into cache trazadone\n",
      "add into cache lorazapam\n",
      "add into cache Hershhenson\n",
      "add into cache Ciporkin\n",
      "add into cache Telectronics\n",
      "add into cache antianemia\n",
      "add into cache BRACED\n",
      "add into cache Wedtech\n",
      "add into cache Eppelmann\n",
      "add into cache A&M\n",
      "add into cache Bockris\n",
      "add into cache Shimson\n",
      "add into cache Journal\\/Europe\n",
      "Inserted 4000 words out of 18485 total.\n",
      "add into cache Savaiko\n",
      "add into cache Multiflow\n",
      "add into cache D'Agosto\n",
      "add into cache CalMat\n",
      "add into cache Vizas\n",
      "add into cache !\n",
      "add into cache #A\n",
      "add into cache #B\n",
      "Inserted 5000 words out of 18485 total.\n",
      "add into cache Regaard\n",
      "add into cache Rearding\n",
      "add into cache proessional\n",
      "add into cache utmosts\n",
      "add into cache \\*\n",
      "add into cache Schmedel\n",
      "Inserted 6000 words out of 18485 total.\n",
      "add into cache Lyneses\n",
      "add into cache Elvekrog\n",
      "add into cache attarcks\n",
      "add into cache Insureres\n",
      "add into cache Asilone\n",
      "add into cache Thrombinar\n",
      "add into cache HomeFed\n",
      "add into cache Galoob\n",
      "add into cache Metatrace\n",
      "add into cache M.A.\n",
      "add into cache D'Arcy\n",
      "add into cache N'T\n",
      "add into cache 'T\n",
      "add into cache N.C\n",
      "add into cache NATION'S\n",
      "add into cache kalega\n",
      "add into cache glamour\n",
      "add into cache Provato\n",
      "add into cache narcotraficantes\n",
      "Inserted 7000 words out of 18485 total.\n",
      "add into cache ###tm\n",
      "add into cache ISC\\/Bunker\n",
      "add into cache Viatech\n",
      "add into cache Ferembal\n",
      "add into cache S.A\n",
      "add into cache erembal\n",
      "add into cache USACafes\n",
      "add into cache AT&T\n",
      "add into cache S&P\n",
      "add into cache Kartalia\n",
      "add into cache #\\/###\n",
      "add into cache REPLICATION\n",
      "add into cache narcokleptocrat\n",
      "add into cache Avdel\n",
      "add into cache Minpeco\n",
      "add into cache dischargable\n",
      "add into cache Insitutional\n",
      "add into cache US$\n",
      "add into cache M$\n",
      "Inserted 8000 words out of 18485 total.\n",
      "add into cache Wames\n",
      "add into cache 'til\n",
      "add into cache Garpian\n",
      "add into cache N.J\n",
      "add into cache Confair\n",
      "add into cache firehoops\n",
      "add into cache Petrolane\n",
      "add into cache Chandross\n",
      "add into cache ##\\/##\n",
      "add into cache #\\/##\n",
      "add into cache Remics\n",
      "add into cache ####\\/####\n",
      "add into cache PANHANDLER\n",
      "add into cache Ormstedt\n",
      "add into cache Lucullan\n",
      "Inserted 9000 words out of 18485 total.\n",
      "add into cache sulphur\n",
      "add into cache Streetspeak\n",
      "add into cache international\\/diversified\n",
      "add into cache fumpered\n",
      "add into cache schmumpered\n",
      "add into cache homeequity\n",
      "add into cache unhocked\n",
      "add into cache Davis\\/Zweig\n",
      "add into cache Unprovable\n",
      "add into cache \\*\\*\n",
      "add into cache B#\n",
      "add into cache Ba#\n",
      "add into cache Micronyx\n",
      "add into cache C$\n",
      "add into cache Caere\n",
      "add into cache electroreality\n",
      "add into cache historicized\n",
      "add into cache lanzador\n",
      "add into cache jonron\n",
      "add into cache buyings\n",
      "Inserted 10000 words out of 18485 total.\n",
      "add into cache a.k.a\n",
      "add into cache Hawaiian\\/Japanese\n",
      "add into cache Keizaikai\n",
      "add into cache BetaWest\n",
      "add into cache Clothestime\n",
      "add into cache L.J.\n",
      "add into cache Alcarria\n",
      "add into cache Volio\n",
      "add into cache U.S.S.R.\n",
      "add into cache radiophonic\n",
      "add into cache Daremblum\n",
      "add into cache BanPonce\n",
      "add into cache Paracchini\n",
      "add into cache BankWatch\n",
      "add into cache Comvik\n",
      "add into cache estuarian\n",
      "add into cache unpolarizing\n",
      "add into cache S&L\n",
      "Inserted 11000 words out of 18485 total.\n",
      "add into cache SoftLetter\n",
      "add into cache Digate\n",
      "add into cache DARMAN'S\n",
      "add into cache MANEUVERS\n",
      "add into cache deficitcutting\n",
      "add into cache WANES\n",
      "add into cache Vauxhill\n",
      "add into cache Amhowitz\n",
      "add into cache MAITRE'D\n",
      "add into cache ALCEE\n",
      "add into cache RESIGNATIONS\n",
      "add into cache OBrion\n",
      "add into cache Boorstyn\n",
      "add into cache and\\/or\n",
      "add into cache district\\/state\n",
      "add into cache districts\\/states\n",
      "add into cache governor\\/Democratic\n",
      "add into cache twotiered\n",
      "Inserted 12000 words out of 18485 total.\n",
      "add into cache Buksbaum\n",
      "add into cache F.S.B.\n",
      "add into cache Texasness\n",
      "add into cache micoprocessors\n",
      "add into cache IranU.S\n",
      "add into cache Westburne\n",
      "add into cache Renzas\n",
      "add into cache elswehere\n",
      "add into cache Aussedat\n",
      "add into cache W.R.\n",
      "add into cache Harbanse\n",
      "add into cache Amdura\n",
      "add into cache CoastAmerica\n",
      "add into cache l'Ouest\n",
      "add into cache Discovision\n",
      "add into cache videodisks\n",
      "add into cache videodisk\n",
      "add into cache SURGED\n",
      "add into cache AFTERSHOCKS\n",
      "add into cache Hajak\n",
      "add into cache artillerists\n",
      "add into cache H&R\n",
      "add into cache Cremonie\n",
      "add into cache minicrash\n",
      "add into cache #%\n",
      "add into cache price\\/earnings\n",
      "add into cache Symbol:HRB\n",
      "add into cache PATOIS\n",
      "add into cache U.S.A\n",
      "add into cache C.D.s\n",
      "add into cache EURODOLLARS\n",
      "add into cache Jiotto\n",
      "add into cache Caspita\n",
      "add into cache Senshukai\n",
      "add into cache HealthVest\n",
      "add into cache Dutch\\/Shell\n",
      "add into cache Verfahrenstechnik\n",
      "add into cache N.V\n",
      "add into cache househld\n",
      "add into cache asseet\n",
      "add into cache Stocks\\/Mutual\n",
      "Inserted 13000 words out of 18485 total.\n",
      "add into cache Zarett\n",
      "add into cache Tremendae\n",
      "add into cache HelmsleySpear\n",
      "add into cache Conradies\n",
      "add into cache Morgenzon\n",
      "add into cache whitewalled\n",
      "add into cache A.G.\n",
      "add into cache DISAPPOINTMENTS\n",
      "add into cache P\\/E\n",
      "add into cache Eiszner\n",
      "add into cache Piszczalski\n",
      "add into cache bargelike\n",
      "add into cache XR#Ti\n",
      "add into cache Merkurs\n",
      "add into cache PENCILS\n",
      "add into cache A.D.L.\n",
      "add into cache coextrude\n",
      "add into cache H.F.\n",
      "add into cache overpurchase\n",
      "add into cache D&B\n",
      "add into cache U.K\n",
      "add into cache JCKC\n",
      "add into cache a.m\n",
      "add into cache ONEZIE\n",
      "add into cache DHAWK\n",
      "add into cache DAYAC\n",
      "add into cache LMEYER\n",
      "add into cache HEYNOW\n",
      "add into cache GR#FLRED\n",
      "add into cache DGAULT\n",
      "add into cache MACPOST\n",
      "add into cache SHIBUMI\n",
      "add into cache JROE\n",
      "add into cache CAROLG\n",
      "add into cache deregulaton\n",
      "add into cache S.G.\n",
      "add into cache marketmaking\n",
      "add into cache Gentility\n",
      "add into cache S.C\n",
      "add into cache Gortari\n",
      "add into cache Erensel\n",
      "add into cache Bapilly\n",
      "Inserted 14000 words out of 18485 total.\n",
      "add into cache FFr\n",
      "add into cache Doorne\n",
      "add into cache D'Amico\n",
      "add into cache Daffynition\n",
      "add into cache Marrill\n",
      "add into cache Datatronic\n",
      "add into cache Pankyo\n",
      "add into cache ODDITIES\n",
      "add into cache L.L.\n",
      "add into cache Amcap\n",
      "add into cache Unamused\n",
      "add into cache F\\/A\n",
      "add into cache Kurlak\n",
      "add into cache R#\n",
      "add into cache D#\n",
      "Inserted 15000 words out of 18485 total.\n",
      "add into cache Adsi\n",
      "add into cache #M\n",
      "add into cache Himont\n",
      "add into cache S.p\n",
      "add into cache IBC\\/Donoghue\n",
      "add into cache EWDB\n",
      "add into cache Euroconvertible\n",
      "add into cache Chiat\\/Day\\/Mojo\n",
      "add into cache BOZELL\n",
      "add into cache AC&R\n",
      "add into cache AC&R\\/CCL\n",
      "add into cache Sibra\n",
      "add into cache 'n'\n",
      "add into cache gentleladies\n",
      "add into cache neighbhorhoods\n",
      "add into cache A.F.\n",
      "add into cache Quotron\n",
      "add into cache Calmat\n",
      "add into cache FCB\\/Leber\n",
      "add into cache Motorfair\n",
      "add into cache Azioni\n",
      "add into cache Manaifatturiera\n",
      "add into cache Seiren\n",
      "add into cache Prizms\n",
      "add into cache DRI\\/McGraw\n",
      "add into cache Kotman\n",
      "add into cache D.C\n",
      "Inserted 16000 words out of 18485 total.\n",
      "add into cache Boccone\n",
      "add into cache Refcorp\n",
      "add into cache G.D.\n",
      "add into cache mortgagebacked\n",
      "add into cache Zaishuo\n",
      "add into cache S$\n",
      "add into cache Queks\n",
      "add into cache Premner\n",
      "add into cache Tirello\n",
      "add into cache MeraBank\n",
      "add into cache Malapai\n",
      "add into cache Trinova\n",
      "add into cache Sasea\n",
      "add into cache SHEARSON\n",
      "add into cache Balcor\n",
      "add into cache Arbitragers\n",
      "add into cache Coatedboard\n",
      "add into cache Kadonada\n",
      "add into cache Aldomet\n",
      "add into cache Dilzem\n",
      "add into cache Ittleson\n",
      "add into cache Giroldi\n",
      "add into cache #th\n",
      "add into cache Wittgreen\n",
      "add into cache reindicting\n",
      "add into cache UPJOHN\n",
      "add into cache NESB\n",
      "add into cache Omnibank\n",
      "add into cache Tockman\n",
      "add into cache Stjernsward\n",
      "add into cache president\\/product\n",
      "add into cache A&W\n",
      "add into cache sanitationists\n",
      "add into cache Sigoloff\n",
      "add into cache Kueneke\n",
      "add into cache Cities\\/ABC\n",
      "add into cache Moertel\n",
      "add into cache M.D\n",
      "add into cache Bickwit\n",
      "add into cache SIMPLIFYING\n",
      "add into cache RAVAGES\n",
      "add into cache JAUNTS\n",
      "add into cache BREADBOX\n",
      "add into cache Damonne\n",
      "add into cache Ludcke\n",
      "add into cache Kafaroff\n",
      "add into cache #\\/###ths\n",
      "add into cache Sooraji\n",
      "add into cache #.\n",
      "add into cache Parsow\n",
      "add into cache R.H.\n",
      "add into cache Opositora\n",
      "add into cache impassiveness\n",
      "add into cache Dinsa\n",
      "add into cache HUGO'S\n",
      "add into cache employerpaid\n",
      "add into cache M.B.A.\n",
      "add into cache Euronotes\n",
      "add into cache recaptilization\n",
      "add into cache outfly\n",
      "add into cache KHAD\\/WAD\n",
      "add into cache Keshtmand\n",
      "add into cache Hekhmatyar\n",
      "add into cache Afghanistan\\/Southwest\n",
      "add into cache Reaganauts\n",
      "add into cache Leninskoye\n",
      "add into cache Zamya\n",
      "add into cache Departmentstore\n",
      "add into cache analyses\n",
      "add into cache spaceborn\n",
      "add into cache VAX\\/VMS\n",
      "add into cache Chestman\n",
      "add into cache A&P\n",
      "add into cache overinclusion\n",
      "add into cache taxlow\n",
      "add into cache B.F.\n",
      "add into cache Intertan\n",
      "add into cache Enviropact\n",
      "add into cache Banxquote\n",
      "add into cache reschedulable\n",
      "add into cache Intelogic\n",
      "add into cache currencny\n",
      "add into cache BLOEDEL\n",
      "add into cache MicroGeneSys\n",
      "add into cache VaxSyn\n",
      "add into cache innoculating\n",
      "add into cache RobertsCorp\n",
      "add into cache N.D\n",
      "add into cache JAGRY\n",
      "add into cache Averae\n",
      "add into cache supermainframe\n",
      "add into cache generalpurpose\n",
      "add into cache UAL'S\n",
      "add into cache SKIDDED\n",
      "add into cache BroadBeach\n",
      "add into cache B\\/T\n",
      "Inserted 17000 words out of 18485 total.\n",
      "add into cache president\\/national\n",
      "Inserted 18000 words out of 18485 total.\n"
     ]
    }
   ],
   "source": [
    "nonmodel_cache = {}\n",
    "word2vec_data = {}\n",
    "\n",
    "count = 1\n",
    "ttl_vocab = len(dataset_vocab.keys())\n",
    "\n",
    "try:\n",
    "    for word, _ in dataset_vocab.items():\n",
    "        if count % 1000 == 0:\n",
    "            print(\"Inserted {} words out of {} total.\".format(count, ttl_vocab))\n",
    "\n",
    "        if word in model:\n",
    "            word2vec_data[word] = model[word]\n",
    "        \n",
    "        elif word in nonmodel_cache:\n",
    "            word2vec_data[word] = nonmodel_cache[word]\n",
    "        \n",
    "        else:\n",
    "            print(\"add into cache {}\".format(word))\n",
    "            nonmodel_cache[word] = np.random.uniform(-0.25, 0.25, 300).astype(np.float32)\n",
    "            word2vec_data[word] = nonmodel_cache[word]\n",
    "\n",
    "        count += 1\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(leveldb_path, \"data.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(word2vec_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.00195312e-01,  4.68750000e-02, -4.10156250e-02, -2.75390625e-01,\n",
       "        8.15429688e-02,  1.90429688e-01, -2.63671875e-01, -7.91015625e-02,\n",
       "        2.81250000e-01,  1.03027344e-01,  1.29882812e-01,  1.53320312e-01,\n",
       "        1.63085938e-01,  3.69140625e-01, -2.61718750e-01,  2.48046875e-01,\n",
       "        4.21875000e-01,  1.65039062e-01, -3.00292969e-02, -1.12792969e-01,\n",
       "        9.96093750e-02,  4.43359375e-01,  2.66113281e-02,  2.81250000e-01,\n",
       "        3.18908691e-03,  7.56835938e-02,  1.41601562e-02, -9.03320312e-02,\n",
       "        2.83203125e-01,  2.77343750e-01, -2.96630859e-02, -7.86132812e-02,\n",
       "        3.47656250e-01, -3.45703125e-01,  2.83203125e-01,  1.00097656e-01,\n",
       "       -4.08203125e-01,  1.80664062e-01, -3.26171875e-01,  1.63574219e-02,\n",
       "       -8.49609375e-02, -2.53906250e-01, -2.30468750e-01,  1.05468750e-01,\n",
       "       -6.03027344e-02, -7.12890625e-02, -2.87109375e-01, -2.40478516e-02,\n",
       "       -1.66992188e-01,  5.15625000e-01, -3.88671875e-01,  3.49609375e-01,\n",
       "        8.25195312e-02,  1.20605469e-01, -3.27148438e-02, -7.22656250e-02,\n",
       "       -7.51953125e-02, -3.49121094e-02, -7.61718750e-02, -1.65039062e-01,\n",
       "        4.56542969e-02,  1.87988281e-02, -1.69921875e-01, -6.93359375e-02,\n",
       "       -1.55273438e-01, -1.01074219e-01, -1.75781250e-01,  1.17675781e-01,\n",
       "        4.10156250e-01, -2.09960938e-02, -5.46875000e-02, -6.91406250e-01,\n",
       "        1.46484375e-01,  6.64062500e-02, -4.34570312e-02,  7.66601562e-02,\n",
       "        5.32226562e-02,  1.19628906e-01,  4.84375000e-01,  3.08593750e-01,\n",
       "       -1.76757812e-01, -5.44433594e-02, -5.00488281e-02, -1.86523438e-01,\n",
       "        1.46484375e-01, -3.82812500e-01,  6.78710938e-02, -1.04370117e-02,\n",
       "       -2.85644531e-02, -3.53515625e-01, -2.67578125e-01,  1.52343750e-01,\n",
       "       -1.19018555e-02, -2.53906250e-01,  1.42822266e-02, -1.95312500e-01,\n",
       "        1.50390625e-01, -8.69140625e-02,  7.47070312e-02, -2.57812500e-01,\n",
       "        3.37890625e-01, -5.10253906e-02,  8.30078125e-02,  1.46484375e-01,\n",
       "       -4.22363281e-02,  2.36328125e-01, -1.81640625e-01,  7.47070312e-02,\n",
       "       -2.02148438e-01, -1.38671875e-01, -4.24804688e-02,  2.21679688e-01,\n",
       "        3.78417969e-02, -9.37500000e-02, -2.57812500e-01, -1.23535156e-01,\n",
       "        2.47802734e-02,  1.70898438e-01, -1.51367188e-01,  8.00781250e-02,\n",
       "       -2.45117188e-01,  1.64062500e-01, -1.67968750e-01,  3.12500000e-01,\n",
       "       -1.52343750e-01, -2.92968750e-01, -6.20117188e-02, -1.57226562e-01,\n",
       "       -1.31835938e-01,  2.00195312e-01, -1.45507812e-01, -1.05468750e-01,\n",
       "       -3.56445312e-02, -1.39648438e-01, -4.44412231e-04,  5.49316406e-02,\n",
       "       -2.86865234e-02, -6.29882812e-02, -1.99218750e-01,  8.97216797e-03,\n",
       "       -1.79687500e-01, -1.32812500e-01, -9.32617188e-02, -2.03125000e-01,\n",
       "       -2.05078125e-01, -2.49023438e-01,  2.18750000e-01,  3.24218750e-01,\n",
       "        2.91015625e-01, -1.89453125e-01, -2.33398438e-01, -2.14843750e-01,\n",
       "        1.08886719e-01,  3.84765625e-01, -9.27734375e-02,  9.71679688e-02,\n",
       "       -5.49316406e-03, -2.67578125e-01,  2.64892578e-02, -2.05078125e-01,\n",
       "        6.93359375e-02, -9.57031250e-02, -1.16699219e-01, -9.66796875e-02,\n",
       "       -1.08886719e-01, -1.38671875e-01, -3.26171875e-01,  2.07519531e-02,\n",
       "       -5.51757812e-02,  3.69140625e-01, -3.14453125e-01, -1.67083740e-03,\n",
       "        8.44726562e-02, -2.48046875e-01,  1.51367188e-01,  1.14257812e-01,\n",
       "        2.22656250e-01, -2.48046875e-01, -1.94335938e-01, -4.08203125e-01,\n",
       "       -1.33789062e-01, -2.85156250e-01, -3.78906250e-01, -3.10546875e-01,\n",
       "       -6.54296875e-02, -1.39648438e-01, -1.49414062e-01, -1.05957031e-01,\n",
       "       -1.42578125e-01,  2.39562988e-03,  7.32421875e-02,  8.30078125e-02,\n",
       "       -1.13281250e-01,  1.57226562e-01, -3.94531250e-01, -2.31445312e-01,\n",
       "        2.09960938e-01,  3.63281250e-01,  3.59375000e-01, -2.73437500e-01,\n",
       "        2.88085938e-02, -1.95312500e-01, -3.67187500e-01, -4.23828125e-01,\n",
       "       -1.26342773e-02, -2.08984375e-01, -3.45703125e-01, -1.44531250e-01,\n",
       "        1.40625000e-01, -1.26953125e-01, -1.73828125e-01,  7.12890625e-02,\n",
       "       -1.01074219e-01,  8.30078125e-02, -3.32031250e-01,  6.68945312e-02,\n",
       "        5.37109375e-02, -1.59179688e-01, -1.66015625e-02,  2.51953125e-01,\n",
       "        1.00708008e-02, -1.36718750e-01, -2.87109375e-01,  9.64355469e-03,\n",
       "       -2.26562500e-01, -1.28906250e-01,  2.10937500e-01, -9.76562500e-02,\n",
       "        1.67846680e-04, -1.04980469e-01, -6.40625000e-01,  1.55273438e-01,\n",
       "        3.22265625e-01,  2.75390625e-01,  5.88378906e-02,  1.81640625e-01,\n",
       "       -8.15429688e-02,  1.11816406e-01, -7.93457031e-03,  3.65234375e-01,\n",
       "       -8.93554688e-02,  2.96875000e-01, -1.40625000e-01, -1.75781250e-02,\n",
       "        1.72851562e-01, -5.22460938e-02, -3.37890625e-01, -5.88378906e-02,\n",
       "        1.26953125e-01, -1.20117188e-01, -3.00781250e-01,  2.40234375e-01,\n",
       "        2.77343750e-01,  3.67187500e-01, -2.92968750e-01,  3.88183594e-02,\n",
       "       -1.98974609e-02, -1.71875000e-01,  3.59375000e-01, -3.73046875e-01,\n",
       "        4.94384766e-03, -2.89062500e-01,  2.74658203e-02,  3.73535156e-02,\n",
       "       -1.99218750e-01,  1.45507812e-01, -2.83203125e-01, -3.04687500e-01,\n",
       "        4.82421875e-01,  5.85937500e-03, -1.25000000e-01,  3.16406250e-01,\n",
       "        2.89306641e-02, -4.57031250e-01, -2.35351562e-01,  9.91210938e-02,\n",
       "        2.13867188e-01, -1.80664062e-01,  1.72851562e-01, -4.44335938e-02,\n",
       "        2.31445312e-01, -1.28784180e-02,  4.10156250e-01,  1.20117188e-01,\n",
       "        1.44531250e-01, -4.90234375e-01, -2.57812500e-01,  9.22851562e-02,\n",
       "        1.83593750e-01,  2.73437500e-01, -3.12500000e-01,  1.54296875e-01,\n",
       "        1.96533203e-02, -4.60937500e-01,  1.11328125e-01, -3.19824219e-02,\n",
       "       -9.27734375e-02,  1.17675781e-01, -6.03027344e-02, -1.53320312e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_data['Confidence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221804"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Confidence', 'NN']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class POSDataset():\n",
    "    \n",
    "    def __init__(self, pkl, dataset, tags_to_index, get_all=False):\n",
    "        self.pkl = pkl\n",
    "        self.inputs = []\n",
    "        self.tags = []\n",
    "        self.ptr = 0   # current index\n",
    "        self.n = 0   # n-gram\n",
    "        self.get_all = get_all\n",
    "        \n",
    "        for pair in dataset:\n",
    "            self.inputs.append(pkl[pair[0]])   # get the embedding\n",
    "            self.tags.append(tags_to_index[pair[1]])   # get the tag index\n",
    "            \n",
    "        self.inputs = np.array(self.inputs, dtype=np.float32)\n",
    "        self.tags = np.eye(len(tags_to_index.keys()))[self.tags]\n",
    "        \n",
    "    def set_n_gram(self, n):\n",
    "        self.n = n\n",
    "        \n",
    "    def minibatch(self, size):\n",
    "        batch_inputs = []\n",
    "        batch_tags = []\n",
    "        \n",
    "        if self.get_all:\n",
    "            counter = 0\n",
    "            while counter < len(self.inputs) - self.n + 1:\n",
    "                batch_inputs.append(self.inputs[counter:counter+self.n].flatten())\n",
    "                batch_tags.append(self.tags[counter + self.n - 1])\n",
    "                counter += 1\n",
    "                \n",
    "        elif self.ptr + size < len(self.inputs) - self.n:\n",
    "            counter = self.ptr\n",
    "            while counter < self.ptr + size:\n",
    "                batch_inputs.append(self.inputs[counter:counter+self.n].flatten())\n",
    "                batch_tags.append(self.tags[counter + self.n - 1])\n",
    "                counter += 1\n",
    "        else:\n",
    "            # TODO: may exist bugs\n",
    "            counter = self.ptr\n",
    "            while counter < len(self.inputs) - self.n + 1:\n",
    "                batch_inputs.append(self.inputs[counter:counter+self.n].flatten())\n",
    "                batch_tags.append(self.tags[counter + self.n - 1])\n",
    "                counter += 1\n",
    "\n",
    "            counter2 = 0\n",
    "            while counter2 < size - counter + self.ptr:\n",
    "                batch_inputs.append(self.inputs[counter2:counter2+self.n].flatten())\n",
    "                batch_tags.append(self.tags[counter2 + self.n - 1])\n",
    "                counter2 += 1\n",
    "\n",
    "        self.ptr = (self.ptr + size) % (len(self.inputs) - self.n)\n",
    "        return np.array(batch_inputs, dtype=np.float32), np.array(batch_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = POSDataset(word2vec_data, train_dataset, tags_to_index)\n",
    "test = POSDataset(word2vec_data, test_dataset, tags_to_index, get_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.00195312e-01,  4.68750000e-02, -4.10156250e-02, -2.75390625e-01,\n",
       "        8.15429688e-02,  1.90429688e-01, -2.63671875e-01, -7.91015625e-02,\n",
       "        2.81250000e-01,  1.03027344e-01,  1.29882812e-01,  1.53320312e-01,\n",
       "        1.63085938e-01,  3.69140625e-01, -2.61718750e-01,  2.48046875e-01,\n",
       "        4.21875000e-01,  1.65039062e-01, -3.00292969e-02, -1.12792969e-01,\n",
       "        9.96093750e-02,  4.43359375e-01,  2.66113281e-02,  2.81250000e-01,\n",
       "        3.18908691e-03,  7.56835938e-02,  1.41601562e-02, -9.03320312e-02,\n",
       "        2.83203125e-01,  2.77343750e-01, -2.96630859e-02, -7.86132812e-02,\n",
       "        3.47656250e-01, -3.45703125e-01,  2.83203125e-01,  1.00097656e-01,\n",
       "       -4.08203125e-01,  1.80664062e-01, -3.26171875e-01,  1.63574219e-02,\n",
       "       -8.49609375e-02, -2.53906250e-01, -2.30468750e-01,  1.05468750e-01,\n",
       "       -6.03027344e-02, -7.12890625e-02, -2.87109375e-01, -2.40478516e-02,\n",
       "       -1.66992188e-01,  5.15625000e-01, -3.88671875e-01,  3.49609375e-01,\n",
       "        8.25195312e-02,  1.20605469e-01, -3.27148438e-02, -7.22656250e-02,\n",
       "       -7.51953125e-02, -3.49121094e-02, -7.61718750e-02, -1.65039062e-01,\n",
       "        4.56542969e-02,  1.87988281e-02, -1.69921875e-01, -6.93359375e-02,\n",
       "       -1.55273438e-01, -1.01074219e-01, -1.75781250e-01,  1.17675781e-01,\n",
       "        4.10156250e-01, -2.09960938e-02, -5.46875000e-02, -6.91406250e-01,\n",
       "        1.46484375e-01,  6.64062500e-02, -4.34570312e-02,  7.66601562e-02,\n",
       "        5.32226562e-02,  1.19628906e-01,  4.84375000e-01,  3.08593750e-01,\n",
       "       -1.76757812e-01, -5.44433594e-02, -5.00488281e-02, -1.86523438e-01,\n",
       "        1.46484375e-01, -3.82812500e-01,  6.78710938e-02, -1.04370117e-02,\n",
       "       -2.85644531e-02, -3.53515625e-01, -2.67578125e-01,  1.52343750e-01,\n",
       "       -1.19018555e-02, -2.53906250e-01,  1.42822266e-02, -1.95312500e-01,\n",
       "        1.50390625e-01, -8.69140625e-02,  7.47070312e-02, -2.57812500e-01,\n",
       "        3.37890625e-01, -5.10253906e-02,  8.30078125e-02,  1.46484375e-01,\n",
       "       -4.22363281e-02,  2.36328125e-01, -1.81640625e-01,  7.47070312e-02,\n",
       "       -2.02148438e-01, -1.38671875e-01, -4.24804688e-02,  2.21679688e-01,\n",
       "        3.78417969e-02, -9.37500000e-02, -2.57812500e-01, -1.23535156e-01,\n",
       "        2.47802734e-02,  1.70898438e-01, -1.51367188e-01,  8.00781250e-02,\n",
       "       -2.45117188e-01,  1.64062500e-01, -1.67968750e-01,  3.12500000e-01,\n",
       "       -1.52343750e-01, -2.92968750e-01, -6.20117188e-02, -1.57226562e-01,\n",
       "       -1.31835938e-01,  2.00195312e-01, -1.45507812e-01, -1.05468750e-01,\n",
       "       -3.56445312e-02, -1.39648438e-01, -4.44412231e-04,  5.49316406e-02,\n",
       "       -2.86865234e-02, -6.29882812e-02, -1.99218750e-01,  8.97216797e-03,\n",
       "       -1.79687500e-01, -1.32812500e-01, -9.32617188e-02, -2.03125000e-01,\n",
       "       -2.05078125e-01, -2.49023438e-01,  2.18750000e-01,  3.24218750e-01,\n",
       "        2.91015625e-01, -1.89453125e-01, -2.33398438e-01, -2.14843750e-01,\n",
       "        1.08886719e-01,  3.84765625e-01, -9.27734375e-02,  9.71679688e-02,\n",
       "       -5.49316406e-03, -2.67578125e-01,  2.64892578e-02, -2.05078125e-01,\n",
       "        6.93359375e-02, -9.57031250e-02, -1.16699219e-01, -9.66796875e-02,\n",
       "       -1.08886719e-01, -1.38671875e-01, -3.26171875e-01,  2.07519531e-02,\n",
       "       -5.51757812e-02,  3.69140625e-01, -3.14453125e-01, -1.67083740e-03,\n",
       "        8.44726562e-02, -2.48046875e-01,  1.51367188e-01,  1.14257812e-01,\n",
       "        2.22656250e-01, -2.48046875e-01, -1.94335938e-01, -4.08203125e-01,\n",
       "       -1.33789062e-01, -2.85156250e-01, -3.78906250e-01, -3.10546875e-01,\n",
       "       -6.54296875e-02, -1.39648438e-01, -1.49414062e-01, -1.05957031e-01,\n",
       "       -1.42578125e-01,  2.39562988e-03,  7.32421875e-02,  8.30078125e-02,\n",
       "       -1.13281250e-01,  1.57226562e-01, -3.94531250e-01, -2.31445312e-01,\n",
       "        2.09960938e-01,  3.63281250e-01,  3.59375000e-01, -2.73437500e-01,\n",
       "        2.88085938e-02, -1.95312500e-01, -3.67187500e-01, -4.23828125e-01,\n",
       "       -1.26342773e-02, -2.08984375e-01, -3.45703125e-01, -1.44531250e-01,\n",
       "        1.40625000e-01, -1.26953125e-01, -1.73828125e-01,  7.12890625e-02,\n",
       "       -1.01074219e-01,  8.30078125e-02, -3.32031250e-01,  6.68945312e-02,\n",
       "        5.37109375e-02, -1.59179688e-01, -1.66015625e-02,  2.51953125e-01,\n",
       "        1.00708008e-02, -1.36718750e-01, -2.87109375e-01,  9.64355469e-03,\n",
       "       -2.26562500e-01, -1.28906250e-01,  2.10937500e-01, -9.76562500e-02,\n",
       "        1.67846680e-04, -1.04980469e-01, -6.40625000e-01,  1.55273438e-01,\n",
       "        3.22265625e-01,  2.75390625e-01,  5.88378906e-02,  1.81640625e-01,\n",
       "       -8.15429688e-02,  1.11816406e-01, -7.93457031e-03,  3.65234375e-01,\n",
       "       -8.93554688e-02,  2.96875000e-01, -1.40625000e-01, -1.75781250e-02,\n",
       "        1.72851562e-01, -5.22460938e-02, -3.37890625e-01, -5.88378906e-02,\n",
       "        1.26953125e-01, -1.20117188e-01, -3.00781250e-01,  2.40234375e-01,\n",
       "        2.77343750e-01,  3.67187500e-01, -2.92968750e-01,  3.88183594e-02,\n",
       "       -1.98974609e-02, -1.71875000e-01,  3.59375000e-01, -3.73046875e-01,\n",
       "        4.94384766e-03, -2.89062500e-01,  2.74658203e-02,  3.73535156e-02,\n",
       "       -1.99218750e-01,  1.45507812e-01, -2.83203125e-01, -3.04687500e-01,\n",
       "        4.82421875e-01,  5.85937500e-03, -1.25000000e-01,  3.16406250e-01,\n",
       "        2.89306641e-02, -4.57031250e-01, -2.35351562e-01,  9.91210938e-02,\n",
       "        2.13867188e-01, -1.80664062e-01,  1.72851562e-01, -4.44335938e-02,\n",
       "        2.31445312e-01, -1.28784180e-02,  4.10156250e-01,  1.20117188e-01,\n",
       "        1.44531250e-01, -4.90234375e-01, -2.57812500e-01,  9.22851562e-02,\n",
       "        1.83593750e-01,  2.73437500e-01, -3.12500000e-01,  1.54296875e-01,\n",
       "        1.96533203e-02, -4.60937500e-01,  1.11328125e-01, -3.19824219e-02,\n",
       "       -9.27734375e-02,  1.17675781e-01, -6.03027344e-02, -1.53320312e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.python import control_flow_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_gram = 3\n",
    "embedding_size = 300   # the same with embedding vector for each words\n",
    "n_hidden_1 = 512\n",
    "n_hidden_2 = 256\n",
    "n_output = len(tags_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_training = False\n",
    "training_epoch = 100 if full_training else 2\n",
    "batch_size = 100 if full_training else 32\n",
    "display_step = 10 if full_training else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_batch_norm(x, n_out, phase_train):\n",
    "    \"\"\"\n",
    "    batch normalization\n",
    "    \"\"\"\n",
    "    beta_init = tf.constant_initializer(value=0., dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1., dtype=tf.float32)\n",
    "    \n",
    "    beta = tf.get_variable(\"beta\", [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable(\"gamma\", [n_out], initializer=gamma_init)\n",
    "    \n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name=\"moments\")\n",
    "    \n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "    mean, var = control_flow_ops.cond(phase_train, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "    \n",
    "    x_r = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x_r, mean, var, beta, gamma, \\\n",
    "                                                        variance_epsilon=1e-3, scale_after_normalization=True)\n",
    "    \n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape, phase_train):\n",
    "    weight_init = tf.random_normal_initializer(stddev=(1.0 / weight_shape[0])**0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    weight = tf.get_variable(\"W\", weight_shape, initializer=weight_init)\n",
    "    bias = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    logits = tf.matmul(input, weight) + bias\n",
    "    return tf.nn.sigmoid(layer_batch_norm(logits, weight_shape[1], phase_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(x, phase_train):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [n_gram * embedding_size, n_hidden_1], [n_hidden_1], phase_train)\n",
    "    \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2], phase_train)\n",
    "    \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [n_hidden_2, n_output], [n_output], phase_train)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    xe = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(xe)\n",
    "    train_summary_opt = tf.summary.scalar(\"train_loss\", loss)\n",
    "    return loss, train_summary_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1, beta2=beta2, epsilon=epsilon, \\\n",
    "                                       use_locking=False, name=\"Adam\")\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    with tf.variable_scope(\"Validation\"):\n",
    "        compare = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "        val_summary_op = tf.summary.scalar(\"validation\", accuracy)\n",
    "        return accuracy, val_summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-gram\n"
     ]
    }
   ],
   "source": [
    "print(\"{}-gram\".format(n_gram))\n",
    "train.set_n_gram(n_gram)\n",
    "test.set_n_gram(n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost: 656.6154117286205\n",
      "epoch: 0, validation error: 0.25795120000839233\n",
      "Then,RB\n",
      "the,DT\n",
      "woman,NN\n",
      ",,,\n",
      "after,IN\n",
      "grabbing,VBG\n",
      "her,NNP\n",
      "umbrella,NN\n",
      ",,,\n",
      "went,VBD\n",
      "to,TO\n",
      "the,DT\n",
      "bank,NN\n",
      "to,NNS\n",
      "deposit,NN\n",
      "her,NNP\n",
      "cash,NN\n",
      ".,.\n",
      "epoch: 1, cost: 626.1850555837154\n",
      "epoch: 1, validation error: 0.3242754340171814\n",
      "Then,IN\n",
      "the,NNP\n",
      "woman,NN\n",
      ",,,\n",
      "after,IN\n",
      "grabbing,VBG\n",
      "her,NNP\n",
      "umbrella,NN\n",
      ",,,\n",
      "went,VBD\n",
      "to,TO\n",
      "the,DT\n",
      "bank,NN\n",
      "to,TO\n",
      "deposit,NN\n",
      "her,NNP\n",
      "cash,NN\n",
      ".,.\n",
      "Training Finished.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"autoencoder_model\"):\n",
    "        x = tf.placeholder(\"float\", [None, n_gram * embedding_size])\n",
    "        y = tf.placeholder(\"float\", [None, n_output])\n",
    "        phase_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        output = network(x, phase_train)\n",
    "        cost, cost_summary = loss(output, y)\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        training_op = training(cost, global_step=global_step)\n",
    "        eval_op, eval_summary = evaluate(output, y)\n",
    "        \n",
    "        summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver(max_to_keep=50)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            train_writer = tf.summary.FileWriter(\"pos_\" + str(n_gram) + \"-gram_logs/\", graph=sess.graph)\n",
    "            val_writer = tf.summary.FileWriter(\"pos_\" + str(n_gram) + \"-gram_logs/\", graph=sess.graph)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # valid data\n",
    "            prefix = [\"She\", \"decided\", \"that\", \"it\", \"was\", \"time\", \"to\", \"leave\", \"home\", \".\"]\n",
    "            sentence = [\"Then\", \"the\", \"woman\", \",\", \"after\", \"grabbing\", \"her\", \"umbrella\", \",\", \"went\", \"to\", \"the\", \"bank\", \"to\", \"deposit\", \"her\", \"cash\", \".\"]\n",
    "            test_str = []\n",
    "            if n_gram > 1:\n",
    "                for word in prefix[1-n_gram:]:\n",
    "                    test_str.append(word2vec_data[word])\n",
    "            for word in sentence:\n",
    "                test_str.append(word2vec_data[word])\n",
    "            test_str = np.array(test_str, dtype=np.float32)\n",
    "            \n",
    "            test_input = []\n",
    "            for i in range(0, len(test_str) - n_gram + 1):\n",
    "                test_input.append(test_str[i:i+n_gram].flatten())\n",
    "                \n",
    "            test_input = np.array(test_input, dtype=np.float32)\n",
    "            \n",
    "            # training\n",
    "            for epoch in range(training_epoch):\n",
    "                avg_cost = 0.\n",
    "                total_batch = int(len(train.inputs) / batch_size)\n",
    "                \n",
    "                # look over the whole batch\n",
    "                for idx in range(total_batch):\n",
    "                    b_x, b_y = train.minibatch(batch_size)\n",
    "                    _, cal_cost, cost_sum = sess.run([training_op, cost, cost_summary], feed_dict={x: b_x, y: b_y, phase_train: True})\n",
    "                    train_writer.add_summary(cost_sum, sess.run(global_step))\n",
    "                    avg_cost += cal_cost / batch_size\n",
    "                    \n",
    "                # display the progress\n",
    "                if epoch % display_step == 0:\n",
    "                    # show validation/testing result\n",
    "                    print(\"epoch: {}, cost: {}\".format(epoch, avg_cost))\n",
    "                    v_x, v_y = test.minibatch(0)   # due to get_all=True\n",
    "                    train_writer.add_summary(cost_sum, sess.run(global_step))\n",
    "                    \n",
    "                    accuracy, val_sum = sess.run([eval_op, eval_summary], feed_dict={x: v_x, y: v_y, phase_train: False})\n",
    "                    val_writer.add_summary(val_sum, sess.run(global_step))\n",
    "                    print(\"epoch: {}, validation error: {}\".format(epoch, (1-accuracy)))\n",
    "                    \n",
    "                    # manual test\n",
    "                    test_output = sess.run(output, feed_dict={x: test_input, phase_train: False})\n",
    "                    tags = []\n",
    "                    for tag_vec in test_output:\n",
    "                        index = np.argmax(tag_vec)\n",
    "                        tags.append(index_to_tags[index])\n",
    "                    \n",
    "                    for word_idx in range(len(sentence)):\n",
    "                        print(\"{},{}\".format(sentence[word_idx], tags[word_idx]))\n",
    "                    \n",
    "                    saver.save(sess, \"pos_\" + str(n_gram) + \"-gram_logs//model-checkpoint-\" + str(epoch+1), global_step=global_step)\n",
    "                    \n",
    "        print(\"Training Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos can refer to the webpage https://www.ibm.com/support/knowledgecenter/zh/SS5RWK_3.5.0/com.ibm.discovery.es.ta.doc/iiysspostagset.htm."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
