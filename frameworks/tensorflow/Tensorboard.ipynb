{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQuickstart to Tensorboard\\nThe dataset is using MNIST handwritten dataset.\\n\\nauthor: Jiankai Wang\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Quickstart to Tensorboard\n",
    "The dataset is using MNIST handwritten dataset.\n",
    "\n",
    "author: Jiankai Wang\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: {}\".format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-62a56c969fbd>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/mnist_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/mnist_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting /tmp/mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/mnist_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters / Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_training = True\n",
    "learning_rate = 1e-2\n",
    "training_epochs = 100 if full_training else 1\n",
    "batch_size = 1000 if full_training else 20\n",
    "display_step = 1\n",
    "log_path = os.path.join(\"/\",\"tmp\",\"tensorboard_log\",\"mnist_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 784      # = 28 (height) * 28 (width)\n",
    "n_hidden_1 = 512   # the neural number in 1st hidden layer\n",
    "n_hidden_2 = 256   # the neural number in 2nd hidden layer\n",
    "n_output = 10      # the output / classification number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_std = (2.0 / weight_shape[0]) ** 0.5                              # weight normalization\n",
    "    w_init = tf.random_normal_initializer(stddev=weight_std)                 # normalize the weight parameters\n",
    "    b_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(name=\"W\", shape=weight_shape, initializer=w_init)\n",
    "    b = tf.get_variable(name=\"b\", shape=bias_shape, initializer=b_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x, [n_input, n_hidden_1], [n_hidden_1])\n",
    "        \n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [n_hidden_1, n_hidden_2], [n_hidden_2])\n",
    "    \n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_2, [n_hidden_2, n_output], [n_output])\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    \"\"\"\n",
    "    output: the logits value from inference\n",
    "    y: the labeling data\n",
    "    \"\"\"\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=output, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, global_step):\n",
    "    \"\"\"\n",
    "    loss: the loss value\n",
    "    global_step: the global training step index\n",
    "    \"\"\"\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    grads = tf.gradients(loss, tf.trainable_variables())\n",
    "    grads = list(zip(grads, tf.trainable_variables()))\n",
    "    apply_grads = optimizer.apply_gradients(grads_and_vars=grads, global_step=global_step)\n",
    "    return grads, apply_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    \"\"\"\n",
    "    output: the logits value from inference\n",
    "    y: the labeling data\n",
    "    \"\"\"\n",
    "    compare = tf.equal(tf.argmax(output, axis=1), tf.argmax(y, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(compare, tf.float32))\n",
    "    tf.summary.scalar(\"eval\", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 'tensorboard --logdir=/tmp/tensorboard_log/' to monitor the training process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "Run 'tensorboard --logdir=/tmp/tensorboard_log/' to monitor the training process.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name mlp/hidden_1/W:0 is illegal; using mlp/hidden_1/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/b:0 is illegal; using mlp/hidden_1/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/W:0 is illegal; using mlp/hidden_2/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/b:0 is illegal; using mlp/hidden_2/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/output/W:0 is illegal; using mlp/output/W_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/output/b:0 is illegal; using mlp/output/b_0 instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/W:0/gradient is illegal; using mlp/hidden_1/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_1/b:0/gradient is illegal; using mlp/hidden_1/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/W:0/gradient is illegal; using mlp/hidden_2/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/hidden_2/b:0/gradient is illegal; using mlp/hidden_2/b_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/output/W:0/gradient is illegal; using mlp/output/W_0/gradient instead.\n",
      "INFO:tensorflow:Summary name mlp/output/b:0/gradient is illegal; using mlp/output/b_0/gradient instead.\n",
      "Epoch: 1, Accuracy: 0.46000000834465027, Vaildation Error: 0.53\n",
      "Epoch: 2, Accuracy: 0.6000000238418579, Vaildation Error: 0.4\n",
      "Epoch: 3, Accuracy: 0.6600000262260437, Vaildation Error: 0.34\n",
      "Epoch: 4, Accuracy: 0.6800000071525574, Vaildation Error: 0.32\n",
      "Epoch: 5, Accuracy: 0.6899999976158142, Vaildation Error: 0.31\n",
      "Epoch: 6, Accuracy: 0.7300000190734863, Vaildation Error: 0.27\n",
      "Epoch: 7, Accuracy: 0.75, Vaildation Error: 0.25\n",
      "Epoch: 8, Accuracy: 0.7599999904632568, Vaildation Error: 0.24\n",
      "Epoch: 9, Accuracy: 0.7699999809265137, Vaildation Error: 0.23\n",
      "Epoch: 10, Accuracy: 0.7699999809265137, Vaildation Error: 0.23\n",
      "Epoch: 11, Accuracy: 0.7799999713897705, Vaildation Error: 0.22\n",
      "Epoch: 12, Accuracy: 0.7799999713897705, Vaildation Error: 0.22\n",
      "Epoch: 13, Accuracy: 0.7900000214576721, Vaildation Error: 0.21\n",
      "Epoch: 14, Accuracy: 0.7900000214576721, Vaildation Error: 0.21\n",
      "Epoch: 15, Accuracy: 0.7900000214576721, Vaildation Error: 0.21\n",
      "Epoch: 16, Accuracy: 0.800000011920929, Vaildation Error: 0.2\n",
      "Epoch: 17, Accuracy: 0.800000011920929, Vaildation Error: 0.2\n",
      "Epoch: 18, Accuracy: 0.800000011920929, Vaildation Error: 0.2\n",
      "Epoch: 19, Accuracy: 0.800000011920929, Vaildation Error: 0.2\n",
      "Epoch: 20, Accuracy: 0.800000011920929, Vaildation Error: 0.2\n",
      "Epoch: 21, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 22, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 23, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 24, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 25, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 26, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 27, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 28, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 29, Accuracy: 0.8100000023841858, Vaildation Error: 0.19\n",
      "Epoch: 30, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 31, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 32, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 33, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 34, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 35, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 36, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 37, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 38, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 39, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 40, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 41, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 42, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 43, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 44, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 45, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 46, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 47, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 48, Accuracy: 0.8199999928474426, Vaildation Error: 0.18\n",
      "Epoch: 49, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 50, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 51, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 52, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 53, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 54, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 55, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 56, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 57, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 58, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 59, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 60, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 61, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 62, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 63, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 64, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 65, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 66, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 67, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 68, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 69, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 70, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 71, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 72, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 73, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 74, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 75, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 76, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 77, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 78, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 79, Accuracy: 0.8299999833106995, Vaildation Error: 0.17\n",
      "Epoch: 80, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 81, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 82, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 83, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 84, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 85, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 86, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 87, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 88, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 89, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 90, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 91, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 92, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 93, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 94, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 95, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 96, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 97, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 98, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 99, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Epoch: 100, Accuracy: 0.8399999737739563, Vaildation Error: 0.16\n",
      "Training finishing.\n",
      "Test Accuracy: 0.8377\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope(\"mlp\"):\n",
    "        x = tf.placeholder(\"float\", [None, 784])  # x is batch input\n",
    "        y = tf.placeholder(\"float\", [None, 10])   # y is output for 10 classification\n",
    "\n",
    "        output = inference(x)   # get the inference result\n",
    "        loss_val = loss(output=output, y=y)   # get the loss\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)   # training step\n",
    "        train_grads, train_opt = training(loss=loss_val, global_step=global_step)   # training body\n",
    "        eval_opt = evaluate(output=output, y=y)   # evaluation result\n",
    "        \n",
    "        # show all training variable info\n",
    "        # may cause summary name error\n",
    "        # INFO:tensorflow:Summary name mlp/hidden_1/W:0 is illegal; using mlp/hidden_1/W_0 instead.\n",
    "        for var in tf.trainable_variables():\n",
    "            tf.summary.histogram(var.name, var)\n",
    "            \n",
    "        # show grads info\n",
    "        for grad, var in train_grads:\n",
    "            tf.summary.histogram(var.name + '/gradient', grad)\n",
    "\n",
    "        init_var = tf.global_variables_initializer()\n",
    "        summary_opt = tf.summary.merge_all()   # merge all summaries\n",
    "        saver = tf.train.Saver()   # for saving checkpoints\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            summary_writer = tf.summary.FileWriter(log_path, graph=sess.graph)   # write the summary \n",
    "            sess.run(init_var)   # initialize all variables\n",
    "\n",
    "            for epoch in range(training_epochs):\n",
    "                avg_loss = 0.\n",
    "                total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "                for idx in range(total_batch):\n",
    "                    batch_x, batch_y = mnist.train.next_batch(batch_size=batch_size)   # get the batch data\n",
    "\n",
    "                    feed_dict_data = {x: batch_x, y: batch_y}\n",
    "                    grads, _ = sess.run([train_grads, train_opt], feed_dict=feed_dict_data)   # run training\n",
    "\n",
    "                    batch_loss = sess.run(loss_val, feed_dict=feed_dict_data)\n",
    "                    avg_loss += batch_loss / total_batch   # calculate the average loss\n",
    "\n",
    "                if epoch % display_step == 0:\n",
    "                    # record log\n",
    "\n",
    "                    feed_dict_val_data = {x: mnist.validation.images, y: mnist.validation.labels}\n",
    "                    acc = sess.run(eval_opt, feed_dict=feed_dict_val_data)   # calculate the accuracy\n",
    "\n",
    "                    print(\"Epoch: {}, Accuracy: {}, Vaildation Error: {}\".format(epoch+1, round(acc,2), round(1-acc,2)))\n",
    "                    tf.summary.scalar(\"validation_accuracy\", acc)  \n",
    "\n",
    "                    summary_str = sess.run(summary_opt, feed_dict=feed_dict_val_data)\n",
    "                    summary_writer.add_summary(summary_str, sess.run(global_step))   # write out the summary\n",
    "\n",
    "                    saver.save(sess, os.path.join(log_path, \"model-checkpoint\"), global_step=global_step)\n",
    "\n",
    "            print(\"Training finishing.\")\n",
    "\n",
    "            feed_dict_test_data = {x: mnist.test.images, y: mnist.test.labels}\n",
    "            acc = sess.run(eval_opt, feed_dict=feed_dict_test_data)   # test result\n",
    "            print(\"Test Accuracy:\",acc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
