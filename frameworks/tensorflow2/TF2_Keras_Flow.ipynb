{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"name":"TF2_Keras_Flow.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PaI7xjMZyAE6","colab_type":"text"},"source":["This notebook introduces how to complete a training process from scratch via `tf.keras` in `Tensorflow 2.x`. Keras in Tensorflow, `tf.keras`, is different from the native Keras (https://github.com/keras-team/keras/). The latter one is built on the top of Tensorflow 1.x and not supporting Tensorflow 2.x. And above all, `tf.keras` APIs are optimized for using Tensorflow as the backend, including parallel training tasks, etc.\n","\n","This notebook can be a template helping you run the whole process training a model from scratch."]},{"cell_type":"code","metadata":{"id":"Ez69LdLTeYvR","colab_type":"code","colab":{}},"source":["!pip install --force tensorflow-gpu==2.0.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4OURqUQBkg8J","colab":{}},"source":["import tensorflow as tf\n","import tensorflow.keras as keras\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from tensorflow.python.client import device_lib\n","\n","print(\"Tensorflow Version: {}\".format(tf.__version__))\n","print(\"Tensorflow.Keras Version: {}\".format(keras.__version__))\n","print(\"Devices: {}\".format(device_lib.list_local_devices()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7MbwMJ_Z8aCx","colab_type":"text"},"source":["# Using TF2.Keras"]},{"cell_type":"markdown","metadata":{"id":"Y1fQk63sQae4","colab_type":"text"},"source":["## Data Processing\n","\n","In this tutorial, we are going to use a flower dataset downloaded from the Google storage (https://github.com/googlecodelabs/tensorflow-for-poets-2). The dataset is composed of 5 types of flowers. Each of them contains around 600 images. "]},{"cell_type":"markdown","metadata":{"id":"6dVmKgMgTMdf","colab_type":"text"},"source":["### Downloading an Image Dataset"]},{"cell_type":"code","metadata":{"id":"i01j8GpmTRew","colab_type":"code","colab":{}},"source":["!curl http://download.tensorflow.org/example_images/flower_photos.tgz -o flower_photos.tgz"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"esyRPkoDTf5y","colab_type":"code","colab":{}},"source":["!tar zxf flower_photos.tgz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1NYR4S2Ty1N","colab_type":"text"},"source":["### Spliting Datasets\n","\n","First of all, the flower dataset is split into three parts, training, validation and test sub-datasets in the ratio of 80:10:10.\n","\n","A file (`./flower_photos/file_list.csv`) is used to save the information of each image in the following formats.\n","```text\n","image_path image_category subdataset_type\n","```\n","\n","Another file (`./flower_photos/label_list.txt`) is used to record the categories."]},{"cell_type":"code","metadata":{"id":"1TmKVRG5TyUY","colab_type":"code","colab":{}},"source":["def split_datasets(path, \n","                   output_file_path, \n","                   output_label_path,\n","                   ratio={'train': 0.8, 'val': 0.1, 'test': 0.1},):\n","  assert os.path.exists(path) and output_file_path != None, \"Lacked necessary information.\"\n","  folders = next(os.walk(path))[1]\n","\n","  for folder in folders:\n","    if folder[0] == \".\": continue\n","\n","    folder_path = os.path.join(path, folder)\n","    filenames = next(os.walk(folder_path))[2]\n","    np.random.shuffle(filenames)\n","    total_file_nums = len(filenames)\n","    train_end = int(total_file_nums * ratio[\"train\"])\n","    val_end = int(total_file_nums * ratio['val'])\n","\n","    with open(output_file_path, \"a\") as fout:\n","      for name in filenames[:train_end]:\n","        file_path = os.path.join(folder_path, name)\n","        fout.write(\"{} {} {}\\n\".format(file_path, folder, \"train\"))\n","      for name in filenames[train_end:(train_end + val_end)]:\n","        file_path = os.path.join(folder_path, name)\n","        fout.write(\"{} {} {}\\n\".format(file_path, folder, \"val\"))\n","      for name in filenames[(train_end + val_end):]:\n","        file_path = os.path.join(folder_path, name)\n","        fout.write(\"{} {} {}\\n\".format(file_path, folder, \"test\"))\n","      \n","  with open(output_label_path, \"w\") as fout:\n","    for folder in folders:\n","      if folder[0] == \".\": continue\n","      fout.write(\"{}\\n\".format(folder))\n","\n","\n","file_list = \"./flower_photos/file_list.csv\"\n","label_list = \"./flower_photos/label_list.txt\"\n","if not os.path.exists(file_list):\n","  filenames = split_datasets(\"./flower_photos\", file_list, label_list)\n","  print(\"Generate both file and label lists.\")\n","else:\n","  print(\"Used a previous generated file.\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WtSRTo0ZcB6e","colab_type":"code","colab":{}},"source":["!head -n 5 ./flower_photos/file_list.csv\n","!tail -n 800 ./flower_photos/file_list.csv | head -n 5\n","!wc -l ./flower_photos/file_list.csv\n","!head -n 10 ./flower_photos/label_list.txt\n","!wc -l ./flower_photos/label_list.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kEa4NYpuTKDO","colab_type":"text"},"source":["### Data Generator\n","\n","After a file list for saving image information was generated, a data generator using the file list was necessary to be instantiated.\n","\n","Because there are three types of sub-datasets, three data generators were instantiated to each of them. \n","\n","Second, in the data generator, several image augment methods are also implemented, including flipping, cropping, etc.\n","\n","In short, you are going to instantiate three data generators to different sub-datasets. These data generators would return a batch of images with the operation of augments and their correct labels during a training or test process."]},{"cell_type":"code","metadata":{"id":"T4C953GSQZ4L","colab_type":"code","colab":{}},"source":["class Image_DataGenerator(keras.utils.Sequence):\n","\n","  __indexes = 0\n","  __file_nums = 0\n","  __list_objs = []  # file_path, category, type\n","  __label_list = []\n","\n","  def __init__(self, file_path=None, label_path=None, datatype='train',\n","               batch_size=32, output_shape=(224, 224),\n","               shuffle=True, aug=True):\n","    \"\"\"(Necessary) Constructor of this generator.\"\"\"\n","    self.__indexes = 0\n","    self.__file_nums = 0\n","    self.__list_objs = []\n","    self.__label_list = []\n","\n","    self.batch_size = batch_size\n","    self.output_shape = output_shape\n","    self.shuffle = shuffle\n","    self.aug = aug\n","    self.file_path = file_path\n","    self.label_path = label_path\n","    self.datatype = datatype\n","\n","    self.__load_file_list()\n","    self.on_epoch_end()  # for shuffling\n","\n","  def __load_file_list(self):\n","    \"\"\"(Optional) Load the sub-dataset file.\"\"\"\n","    with open(self.file_path, \"r\") as fin:\n","      for line in fin:\n","        tmp = line.strip().split(\" \")\n","        if tmp[2] == self.datatype:\n","          self.__list_objs.append([tmp[0], tmp[1]])\n","          self.__file_nums += 1\n","\n","    with open(self.label_path, \"r\") as fin:\n","      for line in fin:\n","        self.__label_list.append(line.strip())\n","      self.__label_list = np.array(self.__label_list)\n","    \n","  def __len__(self):\n","    \"\"\"(Necessary) Returns how many batches to the dataset.\"\"\"\n","    return int(np.floor(self.__file_nums / self.batch_size))\n","  \n","  def __getitem__(self, index):\n","    \"\"\"(Necessary) Generate one batch of data\"\"\"\n","    indexes = self.__indexes[index*self.batch_size : (index+1)*self.batch_size]\n","    file_obj_list = [self.__list_objs[k] for k in indexes]\n","    X, y = self.__data_generation(file_obj_list)\n","    return X, y\n","  \n","  def on_epoch_end(self):\n","    \"\"\"(Necessary) Updates indexes after each epoch\"\"\"\n","    self.__indexes = np.arange(self.__file_nums)\n","    if self.shuffle == True:\n","      np.random.shuffle(self.__indexes)\n","\n","  def example_take(self, count):\n","    \"\"\"(Optional) Fetch an example dataset.\"\"\"\n","    return self.__getitem__(count)\n","      \n","  def __data_generation(self, file_obj_list):\n","    \"\"\"(Necessary) Generates data containing batch_size samples\n","    # X : (n_samples, *dim, n_channels)\n","    \"\"\"\n","    # Initialization\n","    X = np.empty((self.batch_size, *self.output_shape, 3))\n","    y = np.empty((self.batch_size, len(self.__label_list)), dtype=float)\n","\n","    # Generate data\n","    for i, obj in enumerate(file_obj_list):\n","      try:\n","        img_path, img_label = obj\n","\n","        # process the image\n","        X[i,] = self.__preprocess_image_data(img_path)\n","\n","        # preprocess the image info\n","        y[i] = self.__process_data_label(img_label)\n","      except Exception as e:\n","        tf.print(\"\\nLoad image {} went error. {}\\n\".format(ID, e))\n","        continue\n","      \n","    return X, y\n","\n","  def __brightness(self, image, bright=0):\n","    # RGB -> HSV (Hue, Saturation, Value)\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(np.uint16)\n","    hsv[:,:,2] += bright\n","    hsv[:,:,2] = np.minimum(hsv[:,:,2], 255)\n","    hsv[:,:,2] = np.maximum(hsv[:,:,2], 0)\n","    hsv = hsv.astype(np.uint8)\n","    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","  \n","  def __contrast(self, image, value=3.0):\n","    # LAB channel \n","    lab= cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n","\n","    # Splitting the LAB image to different channels\n","    l, a, b = cv2.split(lab)\n","\n","    # Applying CLAHE to L-channel\n","    clahe = cv2.createCLAHE(clipLimit=value, tileGridSize=(8,8))\n","    cl = clahe.apply(l)\n","\n","    # Merge the CLAHE enhanced L-channel with the a and b channel\n","    clab = cv2.merge((cl,a,b))\n","\n","    return cv2.cvtColor(clab, cv2.COLOR_LAB2BGR)\n","  \n","  def __tone(self, image, color_value=0):\n","    # RGB -> HSV (Hue, Saturation, Value)\n","    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n","    hsv = hsv.astype(np.uint16)\n","    hsv[:,:,0] += color_value\n","    hsv[:,:,0] = np.minimum(hsv[:,:,0], 255)\n","    hsv[:,:,0] = np.maximum(hsv[:,:,0], 0)\n","    hsv = hsv.astype(np.uint8)\n","    return cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n","  \n","  def __flip(self, img, opt=\"hor\"):\n","    if opt == \"hor\":\n","      return cv2.flip(img, 1)\n","    elif opt == \"ver\":\n","      return cv2.flip(img, 0)\n","    elif opt == \"both\":\n","      return cv2.flip(img, -1)\n","    else:\n","      return img\n","\n","  def __crop(self, image, scale=0.5):\n","    \"\"\"\n","    description: random crop the image into small sections\n","    \"\"\"\n","    (h, w) = image.shape[:2]\n","    new_h = int(h * 0.5)\n","    new_w = int(w * 0.5)\n","    rand_x = np.random.randint(w * (1-scale))\n","    rand_y = np.random.randint(h * (1-scale))\n","    #print(h, w, new_h, new_w, rand_x, rand_y)\n","    return image[rand_y : (rand_y + new_h), rand_x : (rand_x + new_w)]    \n","\n","  def __shift(self, image, x, y):\n","    m = np.float32([[1,0,x],[0,1,y]])\n","    return cv2.warpAffine(image, m, (image.shape[1], image.shape[0]))\n","\n","  def __preprocess_image_data(self, image_path):\n","    \"\"\"(Optional) Preprocess the image.\"\"\"\n","    try:\n","      img = cv2.imread(image_path)\n","      img = img[:,:,::-1]\n","\n","      if self.aug:\n","        #brightness = np.random.randint(0,60)\n","        #img = self.__brightness(img, bright=brightness)\n","\n","        #contrast = np.random.rand()\n","        #img = self.__contrast(img, value=contrast)\n","\n","        #tone = np.random.randint(0,30)\n","        #img = self.__tone(img, tone)  \n","\n","        # random crop\n","        img = self.__crop(img, 0.95)\n","\n","        # random shift\n","        ver, hor = np.random.randint(-8,8), np.random.randint(-8,8)\n","        img = self.__shift(img, ver, hor)\n","\n","        choice = np.random.choice([\"hor\", \"ver\", \"both\"], size=1)\n","        img = self.__flip(img, choice)\n","\n","      img = cv2.resize(img, self.output_shape)\n","\n","      img = (img - 0.0) / 255.0\n","      return img\n","    except Exception as e:\n","      print(\"Failed in processing image {}. ({})\".format(image_path, e))\n","      return np.zeros((*self.output_shape, 3), dtype=float)\n","\n","  def __process_data_label(self, img_label):\n","    \"\"\"(Optional) Process the label in sparse into an one-hot format.\"\"\"\n","    return (self.__label_list == img_label).astype(float)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uO8GF-8F8E4U","colab_type":"text"},"source":["### An example fetching datasets\n","\n","The following is an example to use the data generator."]},{"cell_type":"code","metadata":{"id":"BkvHPjfo385W","colab_type":"code","colab":{}},"source":["imggen = Image_DataGenerator(file_path=file_list, label_path=label_list, datatype='test', aug=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X1xoz9DE5EIW","colab_type":"code","colab":{}},"source":["imgs, labels = imggen.example_take(1)\n","\n","idx = 0\n","\n","plt.imshow((imgs[idx] * 255.0).astype(int))\n","plt.show()\n","\n","print(\"label: {}\".format(labels[idx]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PWgChuwLON8i","colab_type":"text"},"source":["### Data Generator Instances"]},{"cell_type":"code","metadata":{"id":"Xz7zxxBUORdk","colab_type":"code","colab":{}},"source":["train_params = {\"file_path\": file_list, \"label_path\": label_list, \"datatype\": 'train', \"aug\": True}\n","val_params = {\"file_path\": file_list, \"label_path\": label_list, \"datatype\": 'val', \"aug\": False}\n","test_params = {\"file_path\": file_list, \"label_path\": label_list, \"datatype\": 'test', \"aug\": False}\n","\n","train_generator = Image_DataGenerator(**train_params)\n","val_generator = Image_DataGenerator(**val_params)\n","test_generator = Image_DataGenerator(**test_params)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3lksvpgQfmc","colab_type":"text"},"source":["## Building a Model\n","\n","After preprocessing the datasets, it is time for you to build a neural network. Here, you are going to build a CNN model in `tf.keras` with a custom layer, implementing a skip connection idea."]},{"cell_type":"code","metadata":{"id":"ZkBhyDrUCyjj","colab_type":"code","colab":{}},"source":["class Skipconnection(keras.layers.Layer):\n","  \"\"\"Skipconnection implements a connection across different layers.\n","  \n","  The **`Custom Layer` is not recommended for creating complex or sequential layers.**\n","  \"\"\"\n","\n","  target_shape = None\n","\n","  def __init__(self, target_shape, **kwargs):\n","    \"\"\"Necessary\"\"\"\n","    super(Skipconnection, self).__init__(**kwargs)\n","    self.target_shape = target_shape\n","    self.conv1 = keras.layers.Conv2D(filters=30,\n","                                     kernel_size=(4,4), \n","                                     strides=(4,4), \n","                                     padding='same')\n","    self.conv2 = keras.layers.Conv2D(filters=60, \n","                                     kernel_size=(4,4), \n","                                     strides=(4,4), \n","                                     padding='same')   \n","    self.bn1 = keras.layers.BatchNormalization()\n","    self.bn2 = keras.layers.BatchNormalization()\n","\n","  def get_config(self):\n","    \"\"\"Necessary in exporting models.\n","    \n","    Must be available in Tensorflow 2.0.0.\n","    \"\"\"\n","    config = {\"target_shape\": self.target_shape}\n","    base_config = super(Skipconnection, self).get_config()\n","    return dict(list(base_config.items()) + list(config.items()))\n","\n","  def call(self, upstream, downstream, **kwargs):\n","    \"\"\"Necessary\"\"\"\n","\n","    x = self.conv1(upstream)\n","    x = self.bn1(x)\n","    x = self.conv2(x)\n","    x = self.bn2(x)\n","    sc = keras.layers.Add()([x, downstream])\n","    return sc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SKKW-6T665OK","colab_type":"code","colab":{}},"source":["def build_model(inputs):\n","  \"\"\"The function is built for image classification.\n","  inputs: [None, 224, 224, 3]\n","  \"\"\" \n","\n","  # [None, 224, 224, 16]\n","  x = keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=(1,1), padding='same')(inputs)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)\n","\n","  # [None, 224, 224, 24]\n","  x = keras.layers.Conv2D(filters=24, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  # for skip connection\n","  sc_upstream = keras.layers.Activation('relu')(x)\n","\n","  # [none, 112, 112, 30]\n","  x = keras.layers.Conv2D(filters=30, kernel_size=(3,3), strides=(2,2), padding='same')(sc_upstream)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)\n","\n","  # [none, 112, 112, 36]\n","  x = keras.layers.Conv2D(filters=36, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)    \n","  \n","  # [none, 112, 112, 42]\n","  x = keras.layers.Conv2D(filters=42, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # [none, 56, 56, 42]\n","  x = keras.layers.Conv2D(filters=42, kernel_size=(3,3), strides=(2,2), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)      \n","\n","  # [none, 56, 56, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)     \n","\n","  # [none, 56, 56, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)     \n","\n","  # [none, 56, 56, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)    \n","\n","  # [none, 56, 56, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)     \n","\n","  # [none, 28, 28, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(3,3), strides=(2,2), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)      \n","\n","  # [none, 28, 28, 48]\n","  x = keras.layers.Conv2D(filters=48, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)        \n","\n","  # [none, 28, 28, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(3,3), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # [none, 28, 28, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # [none, 14, 14, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(3,3), strides=(2,2), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # [none, 14, 14, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # implements a skip connection\n","  # sc = keras.layers.Conv2D(filters=30, kernel_size=(4,4), strides=(4,4), padding='same')(sc_upstream)\n","  # sc = keras.layers.BatchNormalization()(sc)\n","  # sc = keras.layers.Conv2D(filters=60, kernel_size=(4,4), strides=(4,4), padding='same')(sc) \n","  # sc = keras.layers.BatchNormalization()(sc)\n","  # sc = keras.layers.Add()([sc, x])\n","\n","  # custom layer\n","  sc = Skipconnection([7,7,60])(sc_upstream, x)\n","\n","  # [none, 7, 7, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(3,3), strides=(2,2), padding='same')(sc)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x) \n","\n","  # [none, 7, 7, 60]\n","  x = keras.layers.Conv2D(filters=60, kernel_size=(5,5), strides=(1,1), padding='same')(x)\n","  x = keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x)   \n","\n","  # [None, 7, 7, 72]\n","  x = keras.layers.Conv2D(filters=72, kernel_size=(1,1), strides=(1,1), padding='same')(x)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  x = keras.layers.Activation('relu')(x) \n","\n","  # one-dimensional feature\n","  x = tf.keras.layers.Flatten()(x)\n","  x = tf.keras.layers.Dense(4096)(x)\n","  x = tf.keras.layers.BatchNormalization()(x)\n","  x = tf.keras.layers.Dense(2048)(x)\n","  feature_extraction = tf.keras.layers.BatchNormalization()(x)\n","\n","  outputs = tf.keras.layers.Dense(5)(feature_extraction)\n","  prob = tf.nn.softmax(outputs)  \n","\n","  return prob"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vla7_sRyE9cx","colab_type":"text"},"source":["After you build a neural network architecture, you need to assign both input and output specifications and then wrap them as a model."]},{"cell_type":"code","metadata":{"id":"P3NvxyEWAids","colab_type":"code","colab":{}},"source":["inputs = tf.keras.Input(shape=(224, 224, 3))\n","outputs = build_model(inputs)\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)\n","model.summary() "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MlgRTP1y_7U_","colab_type":"text"},"source":["### A Model built with TF.Hub\n","\n","In Tensorflow 2, you can easily build a model on the top of the pre-trained model downloaded from TFHub. It is easy to add a final layer, mostly a dense layer representing the number of categories. You can also choose to train the downloaded model or not via the parameter `trainable=[False|True]`.\n"]},{"cell_type":"code","metadata":{"id":"p01CC2QsFWCE","colab_type":"code","colab":{}},"source":["used_hub = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"USuySwaRE60X","colab_type":"code","colab":{}},"source":["if used_hub:\n","\n","  !pip install tensorflow_hub\n","\n","  import tensorflow_hub as hub\n","\n","  model = tf.keras.Sequential([\n","      hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\", \n","                    output_shape=[1280], trainable=False, dtype=float), \n","      tf.keras.layers.Dense(5, activation=\"softmax\")\n","  ])\n","  model.build([None, 224, 224, 3])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsLY3e-NSnwK","colab_type":"text"},"source":["### Let's try the inference.\n","\n","We can do an inference, forward propagation, to the model we just wrapped as a test."]},{"cell_type":"code","metadata":{"id":"O_fjIcDHOIoG","colab_type":"code","colab":{}},"source":["val_data = val_generator.example_take(1)\n","val_data[0].shape, val_data[1].shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ylwnNe8uQzuZ","colab_type":"code","colab":{}},"source":["_untrain_result = model.predict(val_data)\n","_untrain_result.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2laP6RcsSyDg","colab_type":"text"},"source":["### Loss function\n","\n","The model training requires an objectiveness or loss function to do the backpropagation. In Tensorflow 2, it is easier to define the loss function with a decorator `@tf.function`. Now you can easily pass Python float objects to the function to get the loss. (convenient than Tensorflow 1)"]},{"cell_type":"code","metadata":{"id":"66OXfcZxSs60","colab_type":"code","colab":{}},"source":["@tf.function\n","def category_loss(y_true, y_pred):\n","  return tf.reduce_mean(tf.losses.categorical_crossentropy(y_true, y_pred))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sJU294rjTRPe","colab_type":"code","colab":{}},"source":["category_loss(val_data[1], _untrain_result)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hrbiTotS0wN","colab_type":"text"},"source":["### Metrics Functrion\n","\n","You need to define metrics to evaluate the training performance. Here accuracy is used as the metric."]},{"cell_type":"code","metadata":{"id":"y51uypIXS39G","colab_type":"code","colab":{}},"source":["@tf.function\n","def category_accuracy(y_true, y_pred):\n","  true_cls = tf.argmax(y_true, axis=1)\n","  pred_cls = tf.argmax(y_pred, axis=1)\n","  correct = tf.reduce_sum(tf.cast(tf.equal(true_cls, pred_cls), float))\n","  total = tf.cast(tf.shape(y_true)[0], float)\n","  return correct / total"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nTyofzQjUFEJ","colab_type":"code","colab":{}},"source":["category_accuracy(val_data[1], _untrain_result)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sW-9Ran4VT1Q","colab_type":"text"},"source":["### Training Parameters"]},{"cell_type":"code","metadata":{"id":"4ZqVubh1y4Uk","colab_type":"code","colab":{}},"source":["learning_rate = 1e-3\n","epochs = 101"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcyCT80iKFwr","colab_type":"text"},"source":["In the long-time training process, the learning rate is better decreasing slightly for discovering the weights. Here a decay function for the learning rate was defined."]},{"cell_type":"code","metadata":{"id":"VURW9wpDVTlG","colab_type":"code","colab":{}},"source":["def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=15):\n","    def schedule(epoch):\n","        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n","    return tf.keras.callbacks.LearningRateScheduler(schedule)\n","\n","lr_sched = step_decay_schedule(initial_lr=learning_rate, decay_factor=0.75, step_size=5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sPnCdTuJKvbx","colab_type":"text"},"source":["The early stopping function helps developers to stop training earlier to avoid overfitting."]},{"cell_type":"code","metadata":{"id":"ZIbdox9RzXyx","colab_type":"code","colab":{}},"source":["early_stopping = tf.keras.callbacks.EarlyStopping(patience=epochs // 3.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_rc2luBnWjPL","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrkhJwwBLCD4","colab_type":"text"},"source":["You can also use Tensorboard to monitor the training process. In Colab, you can load the magic function to view Tensorboard directly inside the Colab environment."]},{"cell_type":"code","metadata":{"id":"skJys8R4V7oW","colab_type":"code","colab":{}},"source":["tfb_visual = keras.callbacks.TensorBoard(\n","    './log', histogram_freq=0, write_graph=True, write_grads=False, write_images=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwmRb8Ns5Xjz","colab_type":"code","colab":{}},"source":["!rm -rf ./log"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lG8V6yThoHEN","colab":{}},"source":["# magic function in colab\n","%reload_ext tensorboard\n","%tensorboard --logdir \"./log\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASJTqKN4Lmbv","colab_type":"text"},"source":["After data preprocessing, model building, and defining loss function and metrics functions, you now can wrap them and compile the model. The following two scripts are the same effects but one is using custom objects for demonstration."]},{"cell_type":"code","metadata":{"id":"Nmdjb09jWxMn","colab_type":"code","colab":{}},"source":["# predefined loss function and metrics function\n","#model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# an advanced operation with custom objects\n","model.compile(optimizer=optimizer, loss=category_loss, metrics=[category_accuracy])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qKrXhnyhMh_k","colab_type":"text"},"source":["The method `fit_generator` or `fit` is to begin the training process."]},{"cell_type":"code","metadata":{"id":"nJUWINQNWDJw","colab_type":"code","colab":{}},"source":["model.fit_generator(train_generator, epochs=epochs, verbose=1, \n","                    validation_data=val_generator, \n","                    callbacks=[tfb_visual, lr_sched, early_stopping])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k-WPBb6M9IY","colab_type":"text"},"source":["You can simply use the `evaluation()` method of the model to evaluate the model on the test dataset."]},{"cell_type":"code","metadata":{"id":"DKNGZ24qflfR","colab_type":"code","colab":{}},"source":["model.evaluate(test_generator)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bY6EX0Zz9CoG","colab_type":"text"},"source":["## Saving and Loading a Model\n","\n","The backend of Keras is Tensorflow, multiple ways to export or load the model are available on both sides. Here you would understand several ways to manipulate models in Keras and how to interact with the Tensorflow environment. \n","\n","**[Notice] In Tensorflow 2.0 released version, a function named `get_config()` must be defined for exporting or importing the model.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nNfvEnYm5v6V","colab_type":"text"},"source":["### Whole Model\n","\n","[**Keras to Keras**] The first way to export and load models in `tf.Keras` is inherited from the native Keras. You can export and import the model generated as a `.h5` format. Such format is mainly used in `tf.Keras` or native Keras, one way among Keras runtime environment."]},{"cell_type":"code","metadata":{"id":"qtBSYx2Ez8In","colab_type":"code","colab":{}},"source":["latest_model_path = \"./whole_model.h5\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGBlCMJh542W","colab_type":"code","colab":{}},"source":["tf.keras.models.save_model(model, latest_model_path)  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GUhdYNoNffb","colab_type":"code","colab":{}},"source":["model_load_1 = tf.keras.models.load_model(\n","    latest_model_path,\n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xvNlsqLNGwjN","colab_type":"code","colab":{}},"source":["imgs, labels = test_generator.example_take(1)\n","labels[:1], model_load_1.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyX_RbNlzfAv","colab_type":"text"},"source":["### Partial Save"]},{"cell_type":"markdown","metadata":{"id":"OEj3CFDq4XxB","colab_type":"text"},"source":["#### Architecture-only saving\n","\n","[**Keras to Keras**] The second way to save models is to separate the model architecture and the model weight from a whole model file.\n","\n","In addition, you can save the model architecture in different two formats, one is `dict` object and the other is `json` format."]},{"cell_type":"markdown","metadata":{"id":"7x81DnH35F-j","colab_type":"text"},"source":["* dict-based saving"]},{"cell_type":"code","metadata":{"id":"VEmMtdfH4bbv","colab_type":"code","colab":{}},"source":["config = model.get_config()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U3JcOMBb4uV_","colab_type":"code","colab":{}},"source":["dict_model = keras.Model.from_config(\n","    config, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EwnwdiDi5KOb","colab_type":"text"},"source":["* json-based saving"]},{"cell_type":"code","metadata":{"id":"KBSFioL95MfZ","colab_type":"code","colab":{}},"source":["config_json = model.to_json()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sr8AIKPk5U2c","colab_type":"code","colab":{}},"source":["json_model = keras.models.model_from_json(\n","    config_json, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aLBvy9R6I6bR","colab_type":"code","colab":{}},"source":["json_model.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sn4jCXE3KwNI","colab_type":"text"},"source":["#### Weights-only saving\n","\n","In model weights, you can also choose to save weights in Keras format `.h5` or in Tensorflow format (as the SavedModel format)."]},{"cell_type":"code","metadata":{"id":"YA5xGGTTAfTp","colab_type":"code","colab":{}},"source":["weights = model.get_weights()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p8YkawS8AeZH","colab_type":"text"},"source":["You can combine two parts, the model configure (get_config()/from_config()) and the model status (get_weights()/set_weights()), to recreate the model.\n","However, you have to call compile() again before using the model for training."]},{"cell_type":"code","metadata":{"id":"iIBIqbKlKytv","colab_type":"code","colab":{}},"source":["config = model.get_config()\n","weights = model.get_weights()\n","\n","model_load_2 = keras.Model.from_config(\n","    config, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})\n","model_load_2.set_weights(weights)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yW6UG1pYevv","colab_type":"code","colab":{}},"source":["model_load_2.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvL65rImDgfR","colab_type":"text"},"source":["#### Both architecture and weights\n","\n","After you save both parts of the model architecture and weights, you can load both of them and recover the model back.\n","\n","* Save to the disk (local files)."]},{"cell_type":"code","metadata":{"id":"gkB4xz7YDjxt","colab_type":"code","colab":{}},"source":["json_config = model.to_json()\n","model_arch_path = os.path.join(\".\", \"arch.json\")\n","with open(model_arch_path, \"w\") as fout:\n","  fout.write(json_config)\n","\n","# save in h5 format\n","model_weights_path = os.path.join(\".\", \"weights.h5\")\n","model.save_weights(model_weights_path)\n","\n","# save in SavedModel model\n","model_weights_path_tf = os.path.join(\".\", \"model\", \"weights_tf\")\n","model.save_weights(model_weights_path_tf, save_format=\"tf\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCCMJQBdFpQy","colab_type":"text"},"source":["* Load both architecture and weights from the disk.\n","\n"]},{"cell_type":"code","metadata":{"id":"UPHaEgSyFurA","colab_type":"code","colab":{}},"source":["json_config_local = \"\"\n","with open(model_arch_path, \"r\") as fin:\n","  json_config_local = fin.read()\n","  \n","# load a h5 format model  \n","model_load_3 = keras.models.model_from_json(\n","    json_config_local, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})\n","model_load_3.load_weights(model_weights_path)\n","\n","# load a tf format model\n","model_load_4 = keras.models.model_from_json(\n","    json_config_local, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})\n","model_load_4.load_weights(model_weights_path_tf)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lPq9ZVxSLf_2","colab_type":"code","colab":{}},"source":["imgs, labels = test_generator.example_take(1)\n","model_load_3.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVGc3jRrLl-j","colab_type":"code","colab":{}},"source":["model_load_4.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTgT-coXJj2m","colab_type":"text"},"source":["### SavedModel\n","\n","[Keras to Tensorflow] The third way is slightly different from the previous two. You can export the model trained in Keras runtime to the Tensorflow. For now, only one type of saving models in Tensorflow was available for Keras, which is `SavedModel` format."]},{"cell_type":"code","metadata":{"id":"0Qmy_Ny3JsBU","colab_type":"code","colab":{}},"source":["SavedModel_path = os.path.join(\".\", \"saved_models\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ZTyzng_C00I","colab_type":"code","colab":{}},"source":["keras.experimental.export_saved_model(\n","    model, \n","    SavedModel_path, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3dACJi_QDhC1","colab_type":"code","colab":{}},"source":["saved_model = keras.experimental.load_from_saved_model(\n","    SavedModel_path, \n","    custom_objects={'Skipconnection': Skipconnection,\n","                    'category_loss': category_loss, \n","                    'category_accuracy': category_accuracy})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBgEdRr4KWt6","colab_type":"code","colab":{}},"source":["imgs, labels = test_generator.example_take(1)\n","saved_model.predict(imgs[:1])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RnY8qJ1mJpBG","colab_type":"code","colab":{}},"source":["!tar -cf savedmodels.tar saved_models"],"execution_count":0,"outputs":[]}]}