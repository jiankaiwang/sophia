{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TF2Data_TFText.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNmummgLTL2RoXFan+bUoX1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HXaZNd3hEs4W","colab_type":"code","colab":{}},"source":["!pip install -q tf-nightly\n","!pip install -q tensorflow-text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7KDXu0oE1Dq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"8981bf2a-d690-41cd-892f-a302fa31c341","executionInfo":{"status":"ok","timestamp":1578536476151,"user_tz":-480,"elapsed":2560,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["import tensorflow as tf\n","import tensorflow_text as text\n","\n","print(\"Tensorflow Version: {}\".format(tf.__version__))\n","print(\"Eager Mode: {}\".format(tf.executing_eagerly()))\n","print(\"GPU {} available\".format(\"is\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"not\"))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Tensorflow Version: 2.0.0\n","Eager Mode: True\n","GPU not available\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jL2svMwIHxgx","colab_type":"text"},"source":["# Unicode"]},{"cell_type":"markdown","metadata":{"id":"z30Jv8L_IwLH","colab_type":"text"},"source":["Most ops in `TF.Text` expect the text string is UTF-8 encoded. If you use a different encoding, you can transcode into UTF-8 via the `tf.strings.unicode_transcode` API."]},{"cell_type":"code","metadata":{"id":"s5GmBiOLFOF6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"4b73ce66-6392-420e-f19d-857d81a59ecf","executionInfo":{"status":"ok","timestamp":1578536980440,"user_tz":-480,"elapsed":1101,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["docs = tf.constant([u'TF.Text ops expect a string encoded in UTF-8'.encode('UTF-16-BE'), \n","                    u'Sad☹'.encode('UTF-16-BE')])\n","utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')\n","utf8_docs"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: id=1, shape=(2,), dtype=string, numpy=\n","array([b'TF.Text ops expect a string encoded in UTF-8',\n","       b'Sad\\xe2\\x98\\xb9'], dtype=object)>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"JcXYMxipJdaB","colab_type":"text"},"source":["# Tokenization\n","\n","Tokenization is the process of breaking up a sentence into tokens. In general, a token usually represents a word, a number, and punctuation.\n","\n","The main methods are `tokenize` and `tokenize_with_offsets`. In `TF.Text`, lots of tokenizers are already implemented, like `WhitespaceTokenizer`, etc. All of the tokenizers return a `RaggedTensor` to allow the variant length of sentences."]},{"cell_type":"markdown","metadata":{"id":"RtVV6XlSL0Rc","colab_type":"text"},"source":["## WhitespaceTokenizer"]},{"cell_type":"code","metadata":{"id":"or9H4YivIso7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e3694736-4e01-4332-d592-3158cb70dd0d","executionInfo":{"status":"ok","timestamp":1578537418630,"user_tz":-480,"elapsed":1634,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokenizer = text.WhitespaceTokenizer()\n","tokens = tokenizer.tokenize(utf8_docs)\n","print(tokens.to_list())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[b'TF.Text', b'ops', b'expect', b'a', b'string', b'encoded', b'in', b'UTF-8'], [b'Sad\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sgtr-yq0L53s","colab_type":"text"},"source":["## UnicodeScriptTokenizer\n","\n","The UnicodeScriptTokenizer tokenizes a sentence on the Unicode script boundaries. It also tokenizes on white space, punctuation as well."]},{"cell_type":"code","metadata":{"id":"lXkN6HMLKPm_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"c167559f-c088-4060-8360-4e387cd517a7","executionInfo":{"status":"ok","timestamp":1578537939777,"user_tz":-480,"elapsed":1159,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokenizer = text.UnicodeScriptTokenizer()\n","tokens = tokenizer.tokenize(utf8_docs)\n","print(tokens.to_list())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[[b'TF', b'.', b'Text', b'ops', b'expect', b'a', b'string', b'encoded', b'in', b'UTF', b'-8'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pgPDO5KYNHvW","colab_type":"text"},"source":["## Unicode Split\n","\n","When tokenizing a sentence without the whitespce, the most common way is to split it on the character."]},{"cell_type":"code","metadata":{"id":"RWG1rYAuMWn9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3dbfaf34-d26a-45b6-dd4a-ab4e28db4b7f","executionInfo":{"status":"ok","timestamp":1578538332131,"user_tz":-480,"elapsed":1108,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokens = tf.strings.unicode_split([u\"語言處理\".encode(\"UTF-8\")], input_encoding=\"UTF-8\")\n","print(tokens.to_list())"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[[b'\\xe8\\xaa\\x9e', b'\\xe8\\xa8\\x80', b'\\xe8\\x99\\x95', b'\\xe7\\x90\\x86']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-ue9o4EaOmPP","colab_type":"text"},"source":["## Offsets"]},{"cell_type":"markdown","metadata":{"id":"umt0hIoUPQkJ","colab_type":"text"},"source":["It is useful to map back the byte position (offset) of the character in a sentence. In `TF.text`, you can access the method `tokenize_with_offsets ` of a tokenizer to get `tokens`, `offset starts` and `offset limits` in the sentence.\n","\n","* tokens: returns a list of tokens\n","* offset_starts: the byte offset where the token starts\n","* offset_limits: the byte offset where the token ends"]},{"cell_type":"code","metadata":{"id":"3Hs9q6L-N2hG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"6186d66f-4ef7-4c44-b145-ce73263cb766","executionInfo":{"status":"ok","timestamp":1578538826234,"user_tz":-480,"elapsed":1144,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokenizer = text.WhitespaceTokenizer()\n","(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(utf8_docs)\n","print(tokens.to_list())\n","print(offset_starts.to_list())\n","print(offset_limits.to_list())"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[[b'TF.Text', b'ops', b'expect', b'a', b'string', b'encoded', b'in', b'UTF-8'], [b'Sad\\xe2\\x98\\xb9']]\n","[[0, 8, 12, 19, 21, 28, 36, 39], [0]]\n","[[7, 11, 18, 20, 27, 35, 38, 44], [6]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"12zc-TDkQW8E","colab_type":"text"},"source":["## with `TF2.data` APIs\n","\n","Tokenizers are also working with the `tf.data` APIs."]},{"cell_type":"code","metadata":{"id":"vdTCjvtkTIo8","colab_type":"code","colab":{}},"source":["docs = tf.data.Dataset.from_tensor_slices([[u\"Natural Language Processing\"], [u\"語言處理\".encode(\"UTF-8\")]])\n","tokenizer = text.WhitespaceTokenizer()\n","tokenized_docs = docs.map(lambda d: tokenizer.tokenize(d))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"13N2fy4pUbVz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f9a3797d-2c08-4dfa-c9d8-2ae9d4f57f44","executionInfo":{"status":"ok","timestamp":1578540216426,"user_tz":-480,"elapsed":1171,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["docs_iter  = iter(tokenized_docs)\n","print(next(docs_iter).to_list())\n","print(next(docs_iter).to_list())"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[b'Natural', b'Language', b'Processing']]\n","[[b'\\xe8\\xaa\\x9e\\xe8\\xa8\\x80\\xe8\\x99\\x95\\xe7\\x90\\x86']]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oavjTZ9xXHNm","colab_type":"text"},"source":["# Other Ops\n","\n","`TF.text` also provides some useful tools."]},{"cell_type":"markdown","metadata":{"id":"dRUDBgy9XSot","colab_type":"text"},"source":["## Wordshape\n","\n","The `Wordshape` provides a way to do regular expression checks on the sentence for specific properties, like a punctuation character, etc."]},{"cell_type":"code","metadata":{"id":"MJj8V68iUoGJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"outputId":"9bd9a2a0-9db4-4779-9abf-5ef397dfaadf","executionInfo":{"status":"ok","timestamp":1578541000280,"user_tz":-480,"elapsed":1102,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokenier = text.WhitespaceTokenizer()\n","tokens = tokenizer.tokenize(utf8_docs)\n","print(tokens)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["<tf.RaggedTensor [[b'TF.Text', b'ops', b'expect', b'a', b'string', b'encoded', b'in', b'UTF-8'], [b'Sad\\xe2\\x98\\xb9']]>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gzZETREFYB3l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"fb5cdf3d-be6d-4903-90d9-d88129f55aea","executionInfo":{"status":"ok","timestamp":1578541058473,"user_tz":-480,"elapsed":854,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE)\n","print(f1.to_list())"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[False, False, False, False, False, False, False, False], [True]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uu5qWBbpYPi9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"cef8882e-53dc-44b4-d5de-afad8d0bf226","executionInfo":{"status":"ok","timestamp":1578541102693,"user_tz":-480,"elapsed":1114,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE)\n","print(f2.to_list())"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[[False, False, False, False, False, False, False, False], [False]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JYkRvyYwYa55","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b713c548-549e-4cc2-93b5-0aee9fee7922","executionInfo":{"status":"ok","timestamp":1578541130668,"user_tz":-480,"elapsed":1227,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL)\n","print(f3.to_list())"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[[True, False, False, False, False, False, False, True], [True]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"54yn-1TgYh1l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"749be80b-e7a7-425f-e14f-1bb2293dd427","executionInfo":{"status":"ok","timestamp":1578541161380,"user_tz":-480,"elapsed":1635,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE)\n","print(f4.to_list())"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[[False, False, False, False, False, False, False, False], [False]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fGbocfN8YsSc","colab_type":"text"},"source":["## N-Grams & Sliding Window"]},{"cell_type":"markdown","metadata":{"id":"ERKippmwZxAK","colab_type":"text"},"source":["N-grams are a sequential word given a sliding window of n. Some operations are also provided to the N-grams, like `Reduction.STRING_JOIN` or `Reduction.Sum`, etc.  "]},{"cell_type":"code","metadata":{"id":"S2Odj1_qYpSu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c36e0ea3-9b0e-4607-ebbc-57680cc9397f","executionInfo":{"status":"ok","timestamp":1578541516361,"user_tz":-480,"elapsed":1073,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["tokener = text.WhitespaceTokenizer()\n","tokens = tokenizer.tokenize(utf8_docs)\n","print(tokens.to_list())"],"execution_count":24,"outputs":[{"output_type":"stream","text":["[[b'TF.Text', b'ops', b'expect', b'a', b'string', b'encoded', b'in', b'UTF-8'], [b'Sad\\xe2\\x98\\xb9']]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NPPHk7-XZ9iq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3d55f2dd-e551-423b-ceea-a2c3759182da","executionInfo":{"status":"ok","timestamp":1578541574394,"user_tz":-480,"elapsed":1108,"user":{"displayName":"王DevOps","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAzLB0C3whTHAdHpq24UrEWqGtbhJElQxTU5_b_4g=s64","userId":"04300517850278510646"}}},"source":["# N-grams\n","bigrams = text.ngrams(tokens, width=2, reduction_type=text.Reduction.STRING_JOIN)\n","print(bigrams.to_list())"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[[b'TF.Text ops', b'ops expect', b'expect a', b'a string', b'string encoded', b'encoded in', b'in UTF-8'], []]\n"],"name":"stdout"}]}]}